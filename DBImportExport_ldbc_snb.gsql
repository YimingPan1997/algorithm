CREATE GRAPH ldbc_snb(Comment, Post, Company, University, City, Country, Continent, Forum, Person, Tag, TagClass, CONTAINER_OF, HAS_CREATOR, HAS_INTEREST, HAS_MEMBER, HAS_MODERATOR, HAS_TAG, HAS_TYPE, IS_LOCATED_IN, IS_PART_OF, IS_SUBCLASS_OF, KNOWS, LIKES, REPLY_OF, STUDY_AT, WORK_AT, WEIGHT_DOUBLE)
set exit_on_error = "false"
CREATE LOADING JOB load_static_with_header FOR GRAPH ldbc_snb {
      DEFINE FILENAME file_TagClass;
      DEFINE FILENAME file_TagClass_isSubclassOf_TagClass;
      DEFINE FILENAME file_Organisation_isLocatedIn_Place;
      DEFINE FILENAME file_Tag_hasType_TagClass;
      DEFINE FILENAME file_Place_isPartOf_Place;
      DEFINE FILENAME file_Tag;
      DEFINE FILENAME file_Place;
      DEFINE FILENAME file_Organisation;
      LOAD file_Organisation TO VERTEX Company VALUES($0, $2, $3) WHERE $1 == "Company" USING SEPARATOR="|", HEADER="true", EOL="\n";
      LOAD file_Organisation TO VERTEX University VALUES($0, $2, $3) WHERE $1 == "University" USING SEPARATOR="|", HEADER="true", EOL="\n";
      LOAD file_Place TO VERTEX City VALUES($0, $1, $2) WHERE $3 == "City" USING SEPARATOR="|", HEADER="true", EOL="\n";
      LOAD file_Place TO VERTEX Country VALUES($0, $1, $2) WHERE $3 == "Country" USING SEPARATOR="|", HEADER="true", EOL="\n";
      LOAD file_Place TO VERTEX Continent VALUES($0, $1, $2) WHERE $3 == "Continent" USING SEPARATOR="|", HEADER="true", EOL="\n";
      LOAD file_Tag TO VERTEX Tag VALUES($0, $1, $2) USING SEPARATOR="|", HEADER="true", EOL="\n";
      LOAD file_TagClass TO VERTEX TagClass VALUES($0, $1, $2) USING SEPARATOR="|", HEADER="true", EOL="\n";
      LOAD file_Tag_hasType_TagClass TO EDGE HAS_TYPE VALUES($0, $1) USING SEPARATOR="|", HEADER="true", EOL="\n";
      LOAD file_Organisation_isLocatedIn_Place TO EDGE IS_LOCATED_IN VALUES($0 Company, $1 Country) WHERE to_int($1) < 111 USING SEPARATOR="|", HEADER="true", EOL="\n";
      LOAD file_Organisation_isLocatedIn_Place TO EDGE IS_LOCATED_IN VALUES($0 University, $1 City) WHERE to_int($1) > 110 USING SEPARATOR="|", HEADER="true", EOL="\n";
      LOAD file_Place_isPartOf_Place TO EDGE IS_PART_OF VALUES($0 Country, $1 Continent) WHERE to_int($0) < 111 USING SEPARATOR="|", HEADER="true", EOL="\n";
      LOAD file_Place_isPartOf_Place TO EDGE IS_PART_OF VALUES($0 City, $1 Country) WHERE to_int($0) > 110 USING SEPARATOR="|", HEADER="true", EOL="\n";
      LOAD file_TagClass_isSubclassOf_TagClass TO EDGE IS_SUBCLASS_OF VALUES($0, $1) USING SEPARATOR="|", HEADER="true", EOL="\n";
    }

set exit_on_error = "true"
set exit_on_error = "false"
CREATE LOADING JOB load_dynamic_with_header FOR GRAPH ldbc_snb {
      DEFINE FILENAME file_Person_hasInterest_Tag;
      DEFINE FILENAME file_Comment;
      DEFINE FILENAME file_Person_isLocatedIn_City;
      DEFINE FILENAME file_Forum_hasModerator_Person;
      DEFINE FILENAME file_Comment_hasTag_Tag;
      DEFINE FILENAME file_Person_likes_Post;
      DEFINE FILENAME file_Person_workAt_Company;
      DEFINE FILENAME file_Forum;
      DEFINE FILENAME file_Person_knows_Person;
      DEFINE FILENAME file_Person_likes_Comment;
      DEFINE FILENAME file_Post_hasCreator_Person;
      DEFINE FILENAME file_Post_hasTag_Tag;
      DEFINE FILENAME file_Forum_hasMember_Person;
      DEFINE FILENAME file_Forum_hasTag_Tag;
      DEFINE FILENAME file_Person_studyAt_University;
      DEFINE FILENAME file_Comment_replyOf_Post;
      DEFINE FILENAME file_Person;
      DEFINE FILENAME file_Post_isLocatedIn_Country;
      DEFINE FILENAME file_Comment_isLocatedIn_Country;
      DEFINE FILENAME file_Comment_replyOf_Comment;
      DEFINE FILENAME file_Comment_hasCreator_Person;
      DEFINE FILENAME file_Post;
      DEFINE FILENAME file_Forum_containerOf_Post;
      LOAD file_Comment TO VERTEX Comment VALUES($1, $0, $2, $3, $4, $5) USING SEPARATOR="|", HEADER="true", EOL="\n";
      LOAD file_Post TO VERTEX Post VALUES($1, $2, $0, $3, $4, $5, $6, $7) USING SEPARATOR="|", HEADER="true", EOL="\n";
      LOAD file_Forum TO VERTEX Forum VALUES($1, $2, $0) USING SEPARATOR="|", HEADER="true", EOL="\n";
      LOAD file_Person TO VERTEX Person VALUES($1, $2, $3, $4, $5, $0, $6, $7, SPLIT($8, ";"), SPLIT($9, ";")) USING SEPARATOR="|", HEADER="true", EOL="\n";
      LOAD file_Forum_containerOf_Post TO EDGE CONTAINER_OF VALUES($1, $2, $0) USING SEPARATOR="|", HEADER="true", EOL="\n";
      LOAD file_Comment_hasCreator_Person TO EDGE HAS_CREATOR VALUES($1 Comment, $2 Person, $0) USING SEPARATOR="|", HEADER="true", EOL="\n";
      LOAD file_Post_hasCreator_Person TO EDGE HAS_CREATOR VALUES($1 Post, $2 Person, $0) USING SEPARATOR="|", HEADER="true", EOL="\n";
      LOAD file_Person_hasInterest_Tag TO EDGE HAS_INTEREST VALUES($1, $2, $0) USING SEPARATOR="|", HEADER="true", EOL="\n";
      LOAD file_Forum_hasMember_Person TO EDGE HAS_MEMBER VALUES($1, $2, $0) USING SEPARATOR="|", HEADER="true", EOL="\n";
      LOAD file_Forum_hasModerator_Person TO EDGE HAS_MODERATOR VALUES($1, $2, $0) USING SEPARATOR="|", HEADER="true", EOL="\n";
      LOAD file_Comment_hasTag_Tag TO EDGE HAS_TAG VALUES($1 Comment, $2 Tag, $0) USING SEPARATOR="|", HEADER="true", EOL="\n";
      LOAD file_Post_hasTag_Tag TO EDGE HAS_TAG VALUES($1 Post, $2 Tag, $0) USING SEPARATOR="|", HEADER="true", EOL="\n";
      LOAD file_Forum_hasTag_Tag TO EDGE HAS_TAG VALUES($1 Forum, $2 Tag, $0) USING SEPARATOR="|", HEADER="true", EOL="\n";
      LOAD file_Comment_isLocatedIn_Country TO EDGE IS_LOCATED_IN VALUES($1 Comment, $2 Country) USING SEPARATOR="|", HEADER="true", EOL="\n";
      LOAD file_Post_isLocatedIn_Country TO EDGE IS_LOCATED_IN VALUES($1 Post, $2 Country) USING SEPARATOR="|", HEADER="true", EOL="\n";
      LOAD file_Person_isLocatedIn_City TO EDGE IS_LOCATED_IN VALUES($1 Person, $2 City) USING SEPARATOR="|", HEADER="true", EOL="\n";
      LOAD file_Person_knows_Person TO EDGE KNOWS VALUES($1, $2, $0) USING SEPARATOR="|", HEADER="true", EOL="\n";
      LOAD file_Person_likes_Comment TO EDGE LIKES VALUES($1 Person, $2 Comment, $0) USING SEPARATOR="|", HEADER="true", EOL="\n";
      LOAD file_Person_likes_Post TO EDGE LIKES VALUES($1 Person, $2 Post, $0) USING SEPARATOR="|", HEADER="true", EOL="\n";
      LOAD file_Comment_replyOf_Comment TO EDGE REPLY_OF VALUES($1 Comment, $2 Comment, $0) USING SEPARATOR="|", HEADER="true", EOL="\n";
      LOAD file_Comment_replyOf_Post TO EDGE REPLY_OF VALUES($1 Comment, $2 Post, $0) USING SEPARATOR="|", HEADER="true", EOL="\n";
      LOAD file_Person_studyAt_University TO EDGE STUDY_AT VALUES($1, $2, $3, $0) USING SEPARATOR="|", HEADER="true", EOL="\n";
      LOAD file_Person_workAt_Company TO EDGE WORK_AT VALUES($1, $2, $3, $0) USING SEPARATOR="|", HEADER="true", EOL="\n";
    }

set exit_on_error = "true"
set exit_on_error = "false"
CREATE QUERY tg_wcc (SET<STRING> v_type, SET<STRING> e_type, INT output_limit = 100,
 BOOL print_accum = TRUE, STRING result_attr = "", STRING file_path = "") {
/*
 This query identifies the Connected Components (undirected edges). When finished, each
 vertex is assigned an INT label = its component ID number.
  v_type: vertex types to traverse          print_accum: print JSON output
  e_type: edge types to traverse            result_attr: INT attr to store results to
  file_path: file to write CSV output to    display_edges: output edges for visualization
  output_limit: max #vertices to output (-1 = all)  
*/

MinAccum<INT> @min_cc_id = 0;       //each vertex's tentative component id
MapAccum<INT, INT> @@comp_sizes_map;
MapAccum<INT, ListAccum<INT>> @@comp_group_by_size_map;
FILE f(file_path); 

Start = {v_type};

# Initialize: Label each vertex with its own internal ID
S = SELECT x 
    FROM Start:x
    POST-ACCUM x.@min_cc_id = getvid(x);

# Propagate smaller internal IDs until no more ID changes can be Done
WHILE (S.size()>0) DO
    S = SELECT t
        FROM S:s -(e_type:e)- v_type:t
ACCUM t.@min_cc_id += s.@min_cc_id // If s has smaller id than t, copy the id to t
HAVING t.@min_cc_id != t.@min_cc_id';
END;
IF file_path != "" THEN
    f.println("Vertex_ID","Component_ID");
END;

Start = {v_type};
Start = SELECT s 
        FROM Start:s
POST-ACCUM 
    IF result_attr != "" THEN 
        s.setAttr(result_attr, s.@min_cc_id) 
    END,
    
    IF print_accum THEN 
        @@comp_sizes_map += (s.@min_cc_id -> 1) 
    END,
    
    IF file_path != "" THEN 
        f.println(s, s.@min_cc_id) 
    END;
IF print_accum THEN
    IF output_limit >= 0 THEN
        Start = SELECT s 
                FROM Start:s 
                LIMIT output_limit;
    END;
    FOREACH (compId,size) IN @@comp_sizes_map DO
        @@comp_group_by_size_map += (size -> compId);
    END;
    PRINT @@comp_group_by_size_map;
    PRINT @@comp_sizes_map as sizes;
    PRINT Start[Start.@min_cc_id];
END;
}

CREATE QUERY tg_shortest_ss_any_wt (VERTEX source, SET<STRING> v_type, SET<STRING> e_type,
 STRING wt_attr, STRING wt_type, INT output_limit = -1, BOOL print_accum = TRUE,
 STRING result_attr = "", STRING file_path = "", BOOL display_edges = FALSE) {
/*
 Single-source shortest path algorithm, with weight edges, possibly negative.
 From the source vertex, finds the weighted shortest path (FLOAT value).
 The Bellman-Ford algorithm for Single-Source Shortest Path with edge weights,
 is used.  If any loop in the graph has a net negative weight, the algorithm will exit.
 source: start vertex                            print_accum: print JSON output
 v_type: vertex types to traverse                result_attr: INT attr to store results to
 e_type: edge types to traverse                  file_path: file to write CSV output to
 wt_attr: attribute for edge weights             output_limit: max #vertices to output
 wt_type: weight data type (INT,FLOAT,DOUBLE)    display_edges: output edges for visualization
*/

  TYPEDEF TUPLE<FLOAT dist, VERTEX pred> Path_Tuple;    
  HeapAccum<Path_Tuple>(1, dist ASC) @min_path_heap;       # retain 1 shortest path
  ListAccum<STRING> @path_list;                    # shortest path FROM source
  SetAccum<EDGE> @@edge_set;          # list of all edges, if display_edges is needed
  OrAccum @or_visited;
  OrAccum @@or_has_neg_loop;                 # Indicates a negative loop is found
  FILE f(file_path);
  INT iter;
  STRING msg40999 = "There is a loop with negative length. Shortest path is undefined.";
  EXCEPTION neg_loop_excep (40999);

  # Check wt_type parameter
  IF wt_type NOT IN ("INT", "FLOAT", "DOUBLE") THEN
      PRINT "wt_type must be INT, FLOAT, or DOUBLE" AS errMsg;
      RETURN;
  END;

  ##### Initialize #####
  start = {source};
  component = {source};                 # the connected component of the source vertex
  start = SELECT s
    FROM start:s
    POST-ACCUM s.@min_path_heap += Path_Tuple(0, s),
         s.@or_visited = TRUE,
         s.@path_list += to_string(s.id);
  
  ##### Get the connected component #####      
  WHILE start.size() > 0 DO
      start = SELECT t
        FROM start:s -(e_type:e)-> v_type:t
        WHERE NOT t.@or_visited
        ACCUM t.@or_visited = TRUE;
      component = component UNION start;
  END;
  PRINT component.size();
  
  ##### Do N-1 iterations: Consider whether each edge lowers the best-known distance.
  iter = component.size() - 1;    # the max iteration is N-1
  WHILE TRUE LIMIT iter DO 
      tmp = SELECT s
            FROM component:s -(e_type:e)-> v_type:t
      ACCUM 
          IF s.@min_path_heap.size()>0 /*AND s.@minPath.top().dist < GSQL_INT_MAX*1.0*/ THEN
                    CASE wt_type
                        WHEN "INT" THEN
          t.@min_path_heap += Path_Tuple(s.@min_path_heap.top().dist + e.getAttr(wt_attr, "INT"), s)
                        WHEN "FLOAT" THEN
                            t.@min_path_heap += Path_Tuple(s.@min_path_heap.top().dist + e.getAttr(wt_attr, "FLOAT"), s)
                        WHEN "DOUBLE" THEN
                            t.@min_path_heap += Path_Tuple(s.@min_path_heap.top().dist + e.getAttr(wt_attr, "DOUBLE"), s)
                        END
    END;
  END;

  ##### Check for loops with net negative weight #####
  component = SELECT s
        FROM component:s -(e_type:e)-> v_type:t
        ACCUM CASE wt_type
                  WHEN "INT" THEN
          @@or_has_neg_loop +=
          s.@min_path_heap.top().dist + e.getAttr(wt_attr,"INT") < t.@min_path_heap.top().dist
                  WHEN "FLOAT" THEN
                      @@or_has_neg_loop +=
          s.@min_path_heap.top().dist + e.getAttr(wt_attr,"FLOAT") < t.@min_path_heap.top().dist
                  WHEN "DOUBLE" THEN
                      @@or_has_neg_loop +=
          s.@min_path_heap.top().dist + e.getAttr(wt_attr,"DOUBLE") < t.@min_path_heap.top().dist   
                  END;
 
  IF @@or_has_neg_loop THEN        
      RAISE neg_loop_excep (msg40999);
  END;

  ##### Calculate the paths #####
  start = {source};
  tmp = SELECT s
  FROM component:s
  WHERE s != source
  POST-ACCUM s.@or_visited = FALSE;
  WHILE start.size() > 0 LIMIT iter DO     # Limit the number of hops
      start = SELECT t
        FROM start:s -(e_type:e)-> v_type:t
        WHERE NOT t.@or_visited
        ACCUM IF s == t.@min_path_heap.top().pred THEN 
        t.@or_visited = TRUE,
        t.@path_list += s.@path_list,
        t.@path_list += to_string(t.id)
        END;
  END;
  
  ##### Output #####
  IF file_path != "" THEN
      f.println("Vertex_ID","Distance","Shortest_Path");
  END;

  component = SELECT s 
              FROM component:s
        POST-ACCUM 
        IF result_attr != "" THEN 
            s.setAttr(result_attr, s.@min_path_heap.top().dist) 
        END,
        IF file_path != "" THEN 
            f.println(s, s.@min_path_heap.top().dist, s.@path_list) 
        END;    
  IF print_accum THEN
      IF output_limit >= 0 THEN
          component = SELECT s 
                FROM component:s 
          LIMIT output_limit;
      END;
      PRINT component[component.@min_path_heap.top().dist, component.@path_list];
      IF display_edges THEN
          tmp = SELECT s
    FROM component:s -(e_type:e)-> v_type:t
    ACCUM @@edge_set += e;
    PRINT @@edge_set;
      END;
  END;
}

CREATE QUERY tg_tri_count_fast(STRING v_type, STRING e_type) {
# Compute the total number of triangles in the graph
# This algorithm is faster than tri_count but uses additional memory for temporary storage
SumAccum<INT> @@sum_cnt;
SetAccum<VERTEX> @neighbors_set;
      
all = {v_type.*};

# We build up our neighbor lists manually because we'll only build them up on the 2 smaller vertices on a triangle. 

tmp = SELECT t
      FROM all:s-((e_type))-> :t
      WHERE s.outdegree(e_type) > t.outdegree(e_type) OR (s.outdegree(e_type) == t.outdegree(e_type) AND getvid(s) > getvid(t))
      ACCUM t.@neighbors_set += s;

# Here we compute the intersection for 2 points on the triangle.
tmp = SELECT t
      FROM all:s-((e_type))-> :t
      WHERE s != t
      ACCUM @@sum_cnt += COUNT(s.@neighbors_set INTERSECT t.@neighbors_set);
                   
# Divide by 2 because every triangle was counted twice
PRINT @@sum_cnt/2 AS num_triangles;

}

CREATE QUERY tg_louvain(SET<STRING> v_type, SET<STRING> e_type, STRING wt_attr = "weight", INT max_iter = 10, 
  STRING result_attr = "cid", STRING file_path = "", BOOL print_info = FALSE) {
  /*
  louvain community detection algorithm
  add keyword DISTRIBUTED for cluster environment

  Parameters:
  v_type: vertex types to traverse
  e_type: edge types to traverse
  wt_attr: attribute name for edge weights use empty string is graph is unweighted
  wt_attr type is hardcoded to FLOAT INT or DOUBLE can be supported by changing all `e.getAttr(wt_attr, "FLOAT")`
  to `e.getAttr(wt_attr, "INT")` or `e.getAttr(wt_attr, "DOUBLE")`
  * note: when there is a weight attribute missmatch, there may not be an explicit error message
  all print results showing 0 data are present is an indication that there might be a weight attribute missmatch
  
  max_iter: maximum iteration of louvain optimization
  result_attr: attribute name to assign community id results to; use empty string to skip
  file_path: file path to write CSV output to; use empty string to skip
  print_info: print louvain execution info
  */

  TYPEDEF TUPLE <FLOAT deltaQ, FLOAT weight, VERTEX cc> move;
  SumAccum<FLOAT> @sum_ac; #sum of the degrees of all the vertices in community C of the vertex
  ListAccum<VERTEX> @cc_list; #the community center
  SumAccum<FLOAT> @sum_weight; # total weight incident to this vertex
  SumAccum<FLOAT> @sum_cc_weight; # total weight incident to the cc vertex
  MapAccum<VERTEX,SumAccum<FLOAT>> @A_map; #A[c]: sum of the edge weights for the edges in community c
  MaxAccum<move> @max_best_move; # highest dQ, highest -Outdegree, highest cc
  ListAccum<VERTEX> @cm_list;  #community member list
  SumAccum<FLOAT> @@sum_m; # total edge weight
  SumAccum<INT> @sum_outdegree;   # helper variable for outdegree calculation
  SumAccum<INT> @@sum_cc_change;
  MapAccum<INT, SumAccum<INT>> @@community_map;
  MapAccum<INT, SumAccum<INT>> @@community_size_count;
  FILE f(file_path);

  // initialize
  Start = {v_type};
  Start = SELECT s 
          FROM Start:s -(e_type:e)- :t
          ACCUM
              @@sum_m += e.getAttr(wt_attr, "FLOAT")*0.5,
              s.@sum_weight += e.getAttr(wt_attr, "FLOAT")*1.0,
              s.@sum_cc_weight += e.getAttr(wt_attr, "FLOAT")*1.0,
              s.@sum_outdegree += 1
          // mark @cc only for vertices with more than 1 neighbors
          // and only the marked vertices will participate in the actual louvain algorithm
          // the unmorked vertices will be resolved by the vertex following heuristic
          POST-ACCUM
              IF s.@sum_outdegree > 1 THEN 
                  s.@cc_list += s 
              END;
  IF print_info THEN
      PRINT Start.size() AS AllVertexCount;
  END;

  // special @cc update in the first iteration
  Start = SELECT t 
          FROM Start:s -(e_type:e)- :t
          WHERE s.@sum_outdegree > 1 AND t.@sum_outdegree > 1
          ACCUM
              t.@max_best_move += move(e.getAttr(wt_attr, "FLOAT")*1.0 + @@sum_m*t.@sum_weight * 
              (t.@sum_weight - s.@sum_weight), -s.@sum_cc_weight, s.@cc_list.get(0))
          POST-ACCUM
              IF t.@max_best_move.deltaQ > 0 THEN
                  IF -t.@max_best_move.weight < t.@sum_cc_weight THEN
                      t.@cc_list.clear(),
                      t.@cc_list += t.@max_best_move.cc,
                      t.@sum_cc_weight = -t.@max_best_move.weight,
                      @@sum_cc_change += 1
                  ELSE
                      IF -t.@max_best_move.weight == t.@sum_cc_weight AND getvid(t) < getvid(t.@max_best_move.cc)  THEN
                          t.@cc_list.clear(),
                          t.@cc_list += t.@max_best_move.cc,
                          t.@sum_cc_weight = -t.@max_best_move.weight,
                          @@sum_cc_change += 1
                      END
                  END
              END;
  IF print_info THEN
      PRINT @@sum_cc_change AS InitChangeCount;
  END;

  // main loop
  WHILE @@sum_cc_change > 0 LIMIT max_iter DO
      // initialize for iteration
      @@sum_cc_change = 0;
      Start = SELECT s 
              FROM Start:s
              WHERE s.@sum_outdegree > 1
              POST-ACCUM
                  s.@sum_ac = 0,
                  s.@cm_list.clear(),
                  s.@A_map.clear();

      Start = SELECT s 
              FROM Start:s
              ACCUM
                  FOREACH v IN s.@cc_list DO
                      CASE WHEN getvid(v) != -1 THEN 
                          v.@cm_list += s 
                      END
                  END;

      Start = SELECT s 
              FROM Start:s -(e_type:e)- :t
              WHERE t.@sum_outdegree > 1
              ACCUM 
                  s.@A_map += (t.@cc_list.get(0) -> e.getAttr(wt_attr, "FLOAT")*1.0);

      Start = SELECT s 
              FROM Start:s
              ACCUM
                  FOREACH v IN s.@cc_list DO
                      CASE WHEN getvid(v) != -1 THEN 
                          v.@sum_ac += s.@sum_weight 
                      END
                  END;

      Start = SELECT s 
              FROM Start:s
              ACCUM
                  FOREACH v IN s.@cm_list DO
                      CASE WHEN getvid(v) != -1 THEN 
                          v.@sum_ac = s.@sum_ac 
                      END
                  END;

      // compute @max_dQ
      Start = SELECT s 
              FROM Start:s -(e_type:e)- :t
              WHERE t.@sum_outdegree > 1
              ACCUM
                  INT A_s = 0,
                  IF s.@A_map.containsKey(s) THEN 
                      A_s = s.@A_map.get(s) 
                  END,
                  s.@max_best_move += move(s.@A_map.get(t.@cc_list.get(0)) - A_s + 
                  1/@@sum_m*s.@sum_weight*(s.@sum_ac-t.@sum_ac), -t.@sum_cc_weight, t.@cc_list.get(0))
              POST-ACCUM
                  IF s.@max_best_move.deltaQ > 0 THEN
                      IF -s.@max_best_move.weight < s.@sum_cc_weight THEN   // smallest best_move weight < current weight
                          s.@cc_list.clear(),
                          s.@cc_list += s.@max_best_move.cc,
                          s.@sum_cc_weight = -s.@max_best_move.weight,
                          @@sum_cc_change += 1
                      ELSE
                          IF -s.@max_best_move.weight == s.@sum_cc_weight AND getvid(s.@cc_list.get(0)) < getvid(s.@max_best_move.cc)  THEN
                              s.@cc_list.clear(),
                              s.@cc_list += s.@max_best_move.cc,
                              s.@sum_cc_weight = -s.@max_best_move.weight,
                              @@sum_cc_change += 1
                          END
                      END
                  END;
      IF print_info THEN
          PRINT @@sum_cc_change AS IterChangeCount;
      END;
  END;

  // process node with outdegree=1
  // follow the vertex to its neighbor's community
  // if the neighbor also have outdegree=1, mark the two vertices as one community
  Start = {v_type};
  Start = SELECT s 
          FROM Start:s -(e_type:e)- :t
          WHERE s.@sum_outdegree == 1 AND t.@sum_outdegree != 1
          ACCUM 
              s.@cc_list += t.@cc_list.get(0);
  IF print_info THEN
      PRINT Start.size() AS VertexFollowedToCommunity;
  END;

  Start = {v_type};
  Start = SELECT s 
          FROM Start:s -(e_type:e)- :t
          WHERE s.@sum_outdegree == 1 AND t.@sum_outdegree == 1
          ACCUM
              IF getvid(s) <= getvid(t) THEN
                  s.@cc_list += s
              ELSE
                  s.@cc_list += t
              END;
  IF print_info THEN
      PRINT Start.size() AS VertexFollowedToVertex;
  END;

  // process node with outdegree=0
  // assign them to communities containing only itself
  Start = {v_type};
  Start = SELECT s 
          FROM Start:s
          WHERE s.@sum_outdegree == 0
          ACCUM 
              s.@cc_list += s;
  IF print_info THEN
      PRINT Start.size() AS VertexAssignedToItself;
  END;

  // save result
  Start = {v_type};
  Start = SELECT s 
          FROM Start:s
          POST-ACCUM
              IF result_attr != "" THEN 
                  s.setAttr(result_attr, getvid(s.@cc_list.get(0))) 
              END,
              IF file_path != "" THEN 
                  f.println(s, getvid(s.@cc_list.get(0))) 
              END;

  // print result satistic
  IF print_info THEN
      Start = SELECT s 
              FROM Start:s
              WHERE s.@cc_list.size() > 0
              POST-ACCUM
                  @@community_map += (getvid(s.@cc_list.get(0)) -> 1);
      PRINT @@community_map.size() AS FinalCommunityCount;
  END;
}

CREATE QUERY tg_mst(VERTEX opt_source, SET<STRING> v_type, SET<STRING> e_type, STRING wt_attr, STRING wt_type,
  INT max_iter = -1, BOOL print_accum = TRUE, STRING result_attr = "", STRING file_path = "") {
/* 
 Returns a set of edges which form a Minumum Spanning Tree for a connected component. The algorithm
 cans tart either with a user-provided seed vertex or a randomly chosen one.  If you want a set of
 tree which span all the graph's components, use the msf (minimum spanning forest) algorithm.
Parameters:
 opt_source: start vertex (optional)              print_accum: print JSON output
 v_type: vertex types to traverse                result_attr: INT attr to store results to
 e_type: edge types to traverse                  file_path: file to write CSV output to
 wt_attr: attribute for edge weights             max_iter: max iterations/edges (-1 = ALL)
 wt_type: weight data type (INT,FLOAT,DOUBLE)    
*/  
  TYPEDEF TUPLE<VERTEX from_v, VERTEX to_v, EDGE e, FLOAT weight, INT vid> EDGE_WEIGHT;
  HeapAccum<EDGE_WEIGHT>(1, weight ASC, vid ASC) @@chosen_edge_heap; // keep the minimal tuple
  SetAccum<EDGE_WEIGHT> @@mst_set;
  SetAccum<EDGE> @@result_set; 
  OrAccum @or_chosen;
  INT iter_limit;
  FILE f (file_path);
  
  # Check wt_type parameter
  IF wt_type NOT IN ("INT", "FLOAT", "DOUBLE") THEN
      PRINT "wt_type must be INT, FLOAT, or DOUBLE" AS errMsg;
      RETURN;
  END;


  # Pick the start vertex to initialize
  All_v = {v_type}; 
  MSTNodes = SELECT s 
             FROM All_v:s LIMIT 1;
  IF opt_source IS NOT NULL THEN
      MSTNodes = {opt_source};
  END;

  Current = SELECT s 
            FROM MSTNodes:s
    POST-ACCUM s.@or_chosen = true;
  PRINT Current[Current.id] AS Source;

    
  # Find the MST 
  iter_limit = All_v.size();      # set max #iterations
  IF max_iter > 0 THEN
      iter_limit = max_iter;
  END;
  WHILE (Current.size() > 0) LIMIT iter_limit DO
      Current = SELECT t
        FROM MSTNodes:s -(e_type:e)-> v_type:t
        WHERE t.@or_chosen == false    // vertex not in MSTNodes
        ACCUM
            CASE wt_type
        WHEN "INT" THEN
            @@chosen_edge_heap += EDGE_WEIGHT(s, t, e, e.getAttr(wt_attr,"INT"), getvid(t))
        WHEN "FLOAT" THEN
            @@chosen_edge_heap += EDGE_WEIGHT(s, t, e, e.getAttr(wt_attr,"FLOAT"), getvid(t))
        WHEN "DOUBLE" THEN
    @@chosen_edge_heap += EDGE_WEIGHT(s, t, e, e.getAttr(wt_attr,"DOUBLE"), getvid(t))
        END
        POST-ACCUM
            IF t == @@chosen_edge_heap.top().to_v THEN    
        t.@or_chosen = TRUE      // mark the chosen vertex to add into MSTNodes
    END
        HAVING t.@or_chosen == true;

      IF @@chosen_edge_heap.size() > 0 THEN
          IF result_attr != "" THEN
      S = SELECT s
     FROM Current:s -(e_type:e) -> v_type:t
  WHERE t == @@chosen_edge_heap.top().from_v
  ACCUM e.setAttr(result_attr, TRUE);
  END;
  IF file_path != "" THEN
      @@mst_set += @@chosen_edge_heap.top();
  END;
  IF print_accum THEN
      @@result_set += @@chosen_edge_heap.top().e;
  END;
      END;    
      @@chosen_edge_heap.clear();
      MSTNodes = MSTNodes UNION Current;      // update MSTNodes
  END;

  # Output
  IF print_accum THEN
      PRINT @@result_set as mst;
  END;

  IF file_path != "" THEN
      f.println("From", "To", "Weight");
      FOREACH e in @@mst_set DO
          f.println(e.from_v, e.to_v, e.weight);
      END;
  END;
}

CREATE QUERY tg_shortest_ss_no_wt (VERTEX source, SET<STRING> v_type, SET<STRING> e_type, 
  INT output_limit = -1, BOOL print_accum =TRUE, STRING result_attr ="", STRING file_path ="",
  BOOL display_edges =FALSE) {
/*
Single-source shortest path algorithm, with unweighted edges.
From the source vertex, finds the unweighted shortest path (number of hops, INT value)
 source: start vertex                         print_accum: print JSON output
 v_type: vertex types to traverse             result_attr: INT attr to store results to
 e_type: edge types to traverse               file_path: file to write CSV output to
 output_limit: max #vertices to output        display_edges: output edges for visualization
*/

  FILE f(file_path);
  MinAccum<INT> @min_dis;
  OrAccum @or_visited;
  ListAccum<VERTEX> @path_list;
  SetAccum<EDGE> @@edge_set;

  ##### Initialization  #####
  Source = {source};
  Source = SELECT s 
           FROM Source:s
   ACCUM s.@or_visited += true, 
         s.@min_dis = 0,
 s.@path_list = s; 
  ResultSet = {source};

  ##### Calculate distances and paths #####
  WHILE(Source.size()>0) DO
      Source = SELECT t
       FROM Source:s -(e_type:e)-> v_type:t
       WHERE t.@or_visited == false
       ACCUM t.@min_dis += s.@min_dis + 1,
             t.@path_list = s.@path_list + [t],
             t.@or_visited += true
      ORDER BY getvid(t);
      ResultSet = ResultSet UNION Source;
  END;

  IF file_path != "" THEN
      f.println("Vertex_ID","Distance","Shortest_Path");
  END;

  ResultSet = SELECT s 
              FROM ResultSet:s 
              POST-ACCUM 
                  IF result_attr != "" 
      THEN s.setAttr(result_attr, s.@min_dis) 
  END,
                  IF file_path != "" THEN 
      f.println(s, s.@min_dis, s.@path_list)
  END;
  
  IF print_accum THEN
      IF output_limit >= 0 THEN
          ResultSet = SELECT s 
              FROM ResultSet:s 
      LIMIT output_limit;
      END;
      PRINT ResultSet[ResultSet.@min_dis, ResultSet.@path_list];
      IF display_edges THEN
          ResultSet = SELECT s 
              FROM ResultSet:s -(e_type:e)-> v_type:t
                      ACCUM @@edge_set += e;
          PRINT @@edge_set;
      END;
  END;
}

CREATE QUERY tg_scc (SET<STRING> v_type, SET<STRING> e_type, SET<STRING> rev_e_type,
  INT top_k_dist, INT output_limit, INT max_iter = 500, INT iter_wcc = 5, BOOL print_accum = TRUE, STRING result_attr= "", STRING file_path=""){ 
    //INT iter_end_trim = 3
/* This query detects strongly connected components based on the following papers:
 * https://www.sandia.gov/~apinar/papers/irreg00.pdf
 * https://www.sciencedirect.com/science/article/pii/S0743731505000535
 * https://stanford-ppl.github.io/website/papers/sc13-hong.pdf

 * iter: number of iteration of the algorithm
 * iter_wcc: find weakly connected components for the active vertices in this iteration, since the largest sccs are already found after several iterations; usually a small number(3 to 10)
 * top_k_dist: top k result in scc distribution

 * DISTRIBUTED QUERY mode for this query is supported from TG 2.4.
 */
TYPEDEF TUPLE <INT csize, INT num> cluster_num;
MapAccum<INT, INT> @@cluster_size_map, @@cluster_dist_map;
HeapAccum<cluster_num>(top_k_dist, csize DESC) @@cluster_dist_heap;
OrAccum @or_is_forward, @or_is_backward, @or_detached, @or_has_pos_indegree, @or_has_pos_outdegree, @or_wcc_active;
SumAccum<INT> @sum_cid, @sum_vid;
MinAccum<INT> @@min_vid, @min_wcc_id_curr, @min_wcc_id_prev;
SumAccum<STRING> @sum_flag;
MapAccum<INT, MinAccum<INT>> @@f_cid_map, @@b_cid_map, @@n_cid_map, @@s_cid_map;
FILE f (file_path);
INT i = 1;
v_all = {v_type};
tmp(ANY) ={};

active = SELECT s
 FROM v_all:s
 ACCUM 
             s.@sum_vid = getvid(s),
     @@min_vid += getvid(s)
 POST-ACCUM 
             s.@sum_cid = @@min_vid;

WHILE active.size()>0 LIMIT max_iter DO
    WHILE TRUE DO   
        tmp =  SELECT s
       FROM active:s -(e_type:e) -> :t
       WHERE t.@or_detached == FALSE AND s.@sum_cid == t.@sum_cid
       ACCUM 
                   s.@or_has_pos_outdegree = TRUE;

tmp =  SELECT s
       FROM active:s -(rev_e_type:e) -> :t
       WHERE t.@or_detached == FALSE AND s.@sum_cid == t.@sum_cid
       ACCUM s.@or_has_pos_indegree = TRUE;
    
trim_set = SELECT s
   FROM active:s
   WHERE s.@or_has_pos_indegree == FALSE OR s.@or_has_pos_outdegree == FALSE
   ACCUM 
                       s.@or_detached = TRUE,
       s.@sum_cid = s.@sum_vid;

IF trim_set.size() == 0 THEN  // no single SCC anymore, terminate the while loop
    BREAK;
END;
    
active = SELECT s
 FROM active:s 
 WHERE s.@or_detached == FALSE
 ACCUM 
                     @@n_cid_map += (s.@sum_cid -> s.@sum_vid)
 POST-ACCUM 
                     s.@sum_cid = @@n_cid_map.get(s.@sum_cid),
     s.@or_has_pos_indegree = FALSE,
     s.@or_has_pos_outdegree = FALSE; 
@@n_cid_map.clear();
    END;
    //END;
    // get WCC
    IF i == iter_wcc THEN
        active = SELECT s
 FROM active:s
 POST-ACCUM 
                     s.@min_wcc_id_curr = s.@sum_vid,
     s.@min_wcc_id_prev = s.@sum_vid;
        curr = active;
WHILE (curr.size()>0) DO
    curr = SELECT t
           FROM curr:s -((e_type|rev_e_type):e)-> :t
   WHERE s.@sum_cid == t.@sum_cid AND t.@or_detached == FALSE
   ACCUM 
                       t.@min_wcc_id_curr += s.@min_wcc_id_prev // If s has a smaller id than t, copy the id to t
   POST-ACCUM
               CASE WHEN t.@min_wcc_id_prev != t.@min_wcc_id_curr THEN // If t's id has changed
           t.@min_wcc_id_prev = t.@min_wcc_id_curr,
   t.@or_wcc_active = true
       ELSE 
   t.@or_wcc_active = false
       END
       HAVING t.@or_wcc_active == true;       
END;
active = SELECT s
 FROM active:s
 ACCUM 
                     s.@sum_cid = s.@min_wcc_id_curr;
     END;
     i = i + 1;

     pivots = SELECT s
      FROM active:s 
      WHERE s.@sum_cid == s.@sum_vid
      ACCUM 
                  s.@or_is_forward = TRUE,
  s.@or_is_backward = TRUE;

     // mark forward set
     curr = pivots;
     WHILE curr.size()>0 DO
         curr = SELECT t 
FROM curr:s -(e_type:e)->:t  // edge
WHERE t.@or_detached == FALSE AND t.@or_is_forward == FALSE AND s.@sum_cid == t.@sum_cid // not traversed
ACCUM 
                    t.@or_is_forward = TRUE;
     END;

     // mark backward set
     curr = pivots;
     WHILE curr.size()>0 DO
         curr = SELECT t 
FROM curr:s -(rev_e_type:e)->:t  // reverse edge
WHERE t.@or_detached == FALSE AND t.@or_is_backward == FALSE AND s.@sum_cid == t.@sum_cid // not traversed
ACCUM t.@or_is_backward = TRUE;
     END;

     active = SELECT s
      FROM active:s 
      ACCUM 
                  IF s.@or_is_forward == TRUE AND s.@or_is_backward == TRUE THEN  // scc
      s.@or_detached = TRUE,
      @@s_cid_map += (s.@sum_cid -> s.@sum_vid)
  ELSE IF s.@or_is_forward == TRUE THEN  // forward set   
      @@f_cid_map += (s.@sum_cid -> s.@sum_vid)
  ELSE IF s.@or_is_backward == TRUE THEN    // backward set
      @@b_cid_map += (s.@sum_cid -> s.@sum_vid)
  ELSE 
      @@n_cid_map += (s.@sum_cid -> s.@sum_vid)   // null set
  END
      POST-ACCUM 
                  IF s.@or_is_forward == TRUE AND s.@or_is_backward == TRUE THEN //scc
      s.@sum_cid = @@s_cid_map.get(s.@sum_cid)
  END,
  IF s.@or_is_forward == TRUE THEN
      IF s.@or_is_backward == FALSE THEN   // forward set
          s.@sum_cid = @@f_cid_map.get(s.@sum_cid)
      END
  ELSE
      IF s.@or_is_backward == TRUE THEN    // backward set
          s.@sum_cid = @@b_cid_map.get(s.@sum_cid) 
      ELSE                              // null set
  s.@sum_cid = @@n_cid_map.get(s.@sum_cid) 
      END
  END,
  s.@or_is_forward = FALSE,
  s.@or_is_backward = FALSE
      HAVING s.@or_detached == FALSE;

      @@s_cid_map.clear();
      @@f_cid_map.clear();
      @@b_cid_map.clear();
      @@n_cid_map.clear();
END;

// result
v_all = SELECT s
FROM v_all:s 
POST-ACCUM 
    @@cluster_size_map += (s.@sum_cid -> 1);
FOREACH (cid, csize) IN @@cluster_size_map DO
    @@cluster_dist_map += (csize -> 1);
END;

FOREACH (csize, number) IN @@cluster_dist_map DO
    @@cluster_dist_heap += cluster_num(csize, number);
END;
PRINT @@cluster_dist_heap;

IF file_path != "" THEN
    f.println("Vertex_ID","Component_ID");
END;

v_all = SELECT s
    FROM v_all:s 
    POST-ACCUM 
        IF result_attr != "" THEN 
            s.setAttr(result_attr, s.@sum_cid) 
        END,
        IF file_path != "" THEN 
            f.println(s, s.@sum_cid) 
        END
LIMIT output_limit;

IF print_accum THEN
    PRINT v_all[v_all.@sum_cid];
END;
}

CREATE QUERY tg_common_neighbors(VERTEX a, VERTEX b, SET<STRING> e_type, BOOL print_res = TRUE) { 
    /*
    This query calculates the number of common neighbors between two vertices.
    Higher the number, the closer two vertices are.

    Parameters :
        a : Input vertex one
        b : Input vertex two
        e_type: edge types to traverse. If all edge types are desired, pass in "ALL" to the set.
        print_res: Boolean of if you want to print result (True by default)
    */
    avs = {a};
    bvs = {b};

    IF "ALL" NOT IN e_type THEN  # Specific edge types defined
        # Get neighbors of source vertices
        na = SELECT n 
             FROM avs -(e_type)-> :n; 
  
        nb = SELECT n 
             FROM bvs -(e_type)-> :n; 
             
    ELSE  # Use any edge types
        # Get neighbors of source vertices
        na = SELECT n 
             FROM avs -()-> :n; 
             
        nb = SELECT n 
             FROM bvs -()-> :n; 
    END;
    # Get neighbors in common
    u = na INTERSECT nb; 
    
    IF print_res THEN
        PRINT u.size() as closeness; 
    END;
}

CREATE QUERY tg_eigenvector_cent(SET<STRING> v_type, SET<STRING> e_type, INT maxIter = 100, FLOAT convLimit = 0.000001,
    INT top_k = 100, BOOL print_accum = True, STRING result_attr = "",STRING file_path = ""
    ) { 
    /* Compute eigenvector Centrality for each VERTEX. 
    Parameters:
    v_type: vertex types to traverse                 
    e_type: edge types to traverse                   
    maxIter: max iteration
    convLimit: convergence limitation
    top_k: report only this many top scores          print_accum: weather print the result
    result_attr: attribute to write result to        file_path: file to write CSV output to
     */ 
     
    TYPEDEF TUPLE<VERTEX Vertex_ID, FLOAT score> Vertex_Score;
    HeapAccum<Vertex_Score>(top_k, score DESC) @@top_scores_heap;
    SumAccum<FLOAT> @@sum_squares_eigen_values;
    SumAccum<FLOAT> @sum_received_value;
    SumAccum<FLOAT> @sum_eigen_value = 1;
    SumAccum<FLOAT> @@sum_cur_norm_values;
    SumAccum<FLOAT> @@sum_prev_norm_values;
    FLOAT conv_value = 9999;
    FILE f (file_path);
    Start = {v_type};
    WHILE conv_value > convLimit LIMIT maxIter DO
        @@sum_squares_eigen_values = 0;
        @@sum_cur_norm_values = 0;
        V = SELECT s 
            FROM Start:s - (e_type:e) -> v_type:t
            ACCUM t.@sum_received_value += s.@sum_eigen_value
            POST-ACCUM s.@sum_eigen_value = s.@sum_received_value,
                       @@sum_squares_eigen_values += s.@sum_eigen_value * s.@sum_eigen_value,
                       s.@sum_received_value = 0;
        p = SELECT s 
            FROM V:s 
            LIMIT 10;
       
        V = SELECT s 
            FROM V:s
            POST-ACCUM s.@sum_eigen_value = s.@sum_eigen_value / sqrt(@@sum_squares_eigen_values),
                       @@sum_cur_norm_values += s.@sum_eigen_value;
        conv_value = abs(@@sum_cur_norm_values - @@sum_prev_norm_values);
        @@sum_prev_norm_values = @@sum_cur_norm_values;
                             
    END;
    #Output
    IF file_path != "" THEN
        f.println("Vertex_ID", "egien vector");
    END;
    Start = SELECT s 
            FROM Start:s
            ACCUM 
                IF s.@sum_eigen_value==1.0 THEN 
                    s.@sum_eigen_value+=-1 
                END
    POST-ACCUM 
        IF result_attr != "" THEN 
                    s.setAttr(result_attr, s.@sum_eigen_value) 
                END,
      
IF print_accum THEN 
                    @@top_scores_heap += Vertex_Score(s, s.@sum_eigen_value) 
                END,
      
IF file_path != "" THEN 
                    f.println(s, s.@sum_eigen_value) 
                END;

    IF print_accum THEN
        PRINT @@top_scores_heap AS top_scores;
    END;

}

CREATE QUERY tg_maximal_indep_set(STRING v_type, STRING e_type, INT max_iter = 100, BOOL print_accum = TRUE, STRING file_path = ""){ 
    /*
    Maximal Independent Set query only supports one edge type and works only for undirected graphs at the moment (8/12/20).
    */
  
    AndAccum @and_active;
    OrAccum @or_selected;
    MinAccum<INT> @min_vid;
    FILE f(file_path);
    INT iter = 0;
  
    Start = {v_type.*};
    Start = SELECT s 
            FROM Start:s
            ACCUM
                IF s.outdegree(e_type) == 0 THEN
                    s.@or_selected += TRUE,
                    s.@and_active += FALSE
                END
            HAVING s.@and_active;
    
    WHILE Start.size()>0 AND iter<max_iter DO
        Start = SELECT s 
                FROM Start:s
                POST-ACCUM
                    s.@min_vid = 9223372036854775807;
    
        TMP = SELECT s 
              FROM Start:s-(e_type:e)->v_type:t
              WHERE t.@and_active
              ACCUM
                  s.@min_vid += getvid(t);
    
        TMP = SELECT s 
              FROM Start:s
              POST-ACCUM
                  IF getvid(s) < s.@min_vid THEN
                      s.@or_selected += TRUE,
                      s.@and_active += FALSE
                  END
              HAVING s.@or_selected;
    
        TMP = SELECT s 
              FROM TMP:s-(e_type:e)->v_type:t
              ACCUM
                  t.@and_active += FALSE;
    
        Start = SELECT s 
                FROM Start:s 
                WHERE s.@and_active;
        iter = iter+1;   
    END;
  
    IF file_path != "" THEN
      f.println("Vertex");
    END;
  
    Start = {v_type.*};
    Start = SELECT s 
            FROM Start:s 
            WHERE s.@or_selected
            ACCUM 
                IF file_path != "" THEN 
                    f.println(s) 
                END;
                
    IF print_accum THEN
      PRINT Start;
    END;
}

CREATE QUERY tg_pagerank_wt (STRING v_type, STRING e_type, STRING wt_attr,
 FLOAT max_change=0.001, INT max_iter=25, FLOAT damping=0.85, INT top_k = 100,
 BOOL print_accum = TRUE, STRING result_attr =  "", STRING file_path = "",
 BOOL display_edges = FALSE) {
/*
 Compute the pageRank score for each vertex in the GRAPH
 In each iteration, compute a score for each vertex:
     score = (1-damping) + damping*sum(received scores FROM its neighbors).
 The pageRank algorithm stops when either of the following is true:
 a) it reaches max_iter iterations;
 b) the max score change for any vertex compared to the last iteration <= max_change.
 v_type: vertex types to traverse          print_accum: print JSON output
 e_type: edge types to traverse            result_attr: INT attr to store results to
 wt_attr: attribute for edge weights
 max_iter: max #iterations                 file_path: file to write CSV output to
 top_k: #top scores to output              display_edges: output edges for visualization
 max_change: max allowed change between iterations to achieve convergence
 damping: importance of traversal vs. random teleport

 This query supports only taking in a single edge for the time being (8/13/2020).
*/
TYPEDEF TUPLE<VERTEX Vertex_ID, FLOAT score> Vertex_Score;
HeapAccum<Vertex_Score>(top_k, score DESC) @@top_scores_heap;
MaxAccum<FLOAT> @@max_diff = 9999;    # max score change in an iteration
SumAccum<FLOAT> @sum_recvd_score = 0; # sum of scores each vertex receives FROM neighbors
SumAccum<FLOAT> @sum_score = 1;           # initial score for every vertex is 1.
SetAccum<EDGE> @@edge_set;             # list of all edges, if display is needed
SumAccum<FLOAT> @sum_total_wt;
FILE f (file_path);

Start = {v_type};
 # Calculate the total weight for each vertex
Start = SELECT s                
        FROM Start:s -(e_type:e) -> v_type:t
        ACCUM s.@sum_total_wt += e.getAttr(wt_attr, "FLOAT"); 
            
# PageRank iterations
# Start with all vertices of specified type(s)
WHILE @@max_diff > max_change LIMIT max_iter DO
    @@max_diff = 0;
    V = SELECT s
FROM Start:s -(e_type:e)-> v_type:t
ACCUM t.@sum_recvd_score += s.@sum_score * e.getAttr(wt_attr, "FLOAT")/s.@sum_total_wt
POST-ACCUM s.@sum_score = (1.0-damping) + damping * s.@sum_recvd_score,
   s.@sum_recvd_score = 0,
   @@max_diff += abs(s.@sum_score - s.@sum_score');
END; # END WHILE loop
# Output
IF file_path != "" THEN
    f.println("Vertex_ID", "PageRank");
END;

V = SELECT s 
    FROM Start:s
    POST-ACCUM 
        IF result_attr != "" THEN 
            s.setAttr(result_attr, s.@sum_score) 
        END,
   
IF file_path != "" THEN 
            f.println(s, s.@sum_score) 
        END,
   
IF print_accum THEN 
            @@top_scores_heap += Vertex_Score(s, s.@sum_score) 
        END;

IF print_accum THEN
    PRINT @@top_scores_heap;
    IF display_edges THEN
        PRINT Start[Start.@sum_score];
Start = SELECT s
FROM Start:s -(e_type:e)-> v_type:t
ACCUM @@edge_set += e;
PRINT @@edge_set;
    END;
END;
}

CREATE QUERY test(/* Parameters here */) FOR GRAPH ldbc_snb { 
  /* Write query logic here */ 
  PRINT "test works!"; 
}
CREATE QUERY tg_harmonic_cent(SET<STRING> v_type, SET<STRING> e_type, SET<STRING> re_type,INT max_hops = 10,
  INT top_k = 100, BOOL wf = TRUE, BOOL print_accum = True, STRING result_attr = "",
  STRING file_path = "", BOOL display_edges = FALSE){ 
  /* Compute Harmonic Centrality for each VERTEX. 
  Use multi-sourse BFS.
  Parameters:
  v_type: vertex types to traverse                 print_accum: print JSON output
  e_type: edge types to traverse                   result_attr: INT attr to store results to
  re_type: reverse edge type in directed graph, in undirected graph set re_type=e_type
  max_hops: look only this far from each vertex    file_path: file to write CSV output to
  top_k: report only this many top scores          display_edges: output edges for visualization
  wf: Wasserman and Faust normalization factor for multi-component graphs */ 
  
  TYPEDEF TUPLE<VERTEX Vertex_ID, FLOAT score> Vertex_Score; #tuple to store harmonic centrality score
  HeapAccum<Vertex_Score>(top_k, score DESC) @@top_scores_heap; #heap to store top K score
  SumAccum<INT> @@sum_curr_dist; #current distance
  BitwiseOrAccum @bitwise_or_visit_next; #use bitwise instead of setAccum
  SumAccum<FLOAT> @sum_res; #Result, sum of distance
  SumAccum<INT> @sum_size; #get graph size
  SumAccum<FLOAT> @sum_score;
  BitwiseOrAccum @bitwise_or_seen;
  BitwiseOrAccum @bitwise_or_visit; 
  SumAccum<INT> @@sum_count = 1;#used to set unique ID
  SumAccum<INT> @sum_id; #store the unique ID
  SetAccum<INT> @@batch_set; #used to set unique ID
  MapAccum<INT,INT> @@map; #used to set unique ID 
  SetAccum<EDGE> @@edge_set;
  INT empty=0;
  FILE f (file_path);
  INT num_vert;
  INT batch_number;
# Compute harmonic
  all = {v_type};
  
  num_vert = all.size();
  batch_number = num_vert/60;
  IF batch_number==0 THEN 
      batch_number=1; 
  END;
    
  #Calculate the sum of distance to other vertex for each vertex
  FOREACH i IN RANGE[0, batch_number-1] DO
      Start = SELECT s 
              FROM all:s
              WHERE getvid(s)%batch_number == i
              POST-ACCUM @@map+=(getvid(s)->0),
                         @@batch_set+=getvid(s);
  
      FOREACH ver in @@batch_set DO 
          @@map+=(ver->@@sum_count); @@sum_count+=1;
      END; #set a unique ID for each vertex, ID from 1-63
    
      Start = SELECT s 
              FROM Start:s 
              POST-ACCUM s.@sum_id=@@map.get(getvid(s));
      Start = Select s 
              FROM Start:s
              POST-ACCUM s.@bitwise_or_seen=1<<s.@sum_id,
                         s.@bitwise_or_visit=1<<s.@sum_id; # set initial seen and visit s.@seen1 s.@seen2 
      @@batch_set.clear();
      @@map.clear();
      @@sum_count=0;
      
      WHILE (Start.size() > 0) LIMIT max_hops DO
          @@sum_curr_dist+=1;
          Start = SELECT t 
                  FROM Start:s -(re_type:e)-v_type:t
                  WHERE s.@bitwise_or_visit&-t.@bitwise_or_seen-1>0 and s!=t #use -t.@seen-1 to get the trverse of t.@seen
                  ACCUM
                      INT c = s.@bitwise_or_visit&-t.@bitwise_or_seen-1,
                      IF c>0 THEN
                          t.@bitwise_or_visit_next+=c,
                          t.@bitwise_or_seen+=c
                      END
                  POST-ACCUM
                      t.@bitwise_or_visit=t.@bitwise_or_visit_next,
                      INT r = t.@bitwise_or_visit_next,
                      WHILE r>0 DO 
                          r=r&(r-1),
                          t.@sum_res+=1.0/@@sum_curr_dist*1.0,
                          t.@sum_size+=1 #count how many 1 in the number, same as setAccum,size()
                      END,
                      t.@bitwise_or_visit_next=0;
      END;
      @@sum_curr_dist=0;
      Start = SELECT s 
              FROM all:s 
              POST-ACCUM s.@bitwise_or_seen=0,
                         s.@bitwise_or_visit=0;
  END;
  
  Start = SELECT s 
          FROM all:s
  # Calculate harmonic Centrality for each vertex
          WHERE s.@sum_res>0
          POST-ACCUM 
              IF wf THEN 
                  s.@sum_score = s.@sum_res*1.0/s.@sum_size*1.0
              ELSE
                  s.@sum_score = s.@sum_res*1.0 
              END,
    
      IF result_attr != "" THEN 
                  s.setAttr(result_attr, s.@sum_score) 
              END,
    
      IF print_accum THEN 
                  @@top_scores_heap += Vertex_Score(s, s.@sum_score) 
              END,
    
      IF file_path != "" THEN 
                  f.println(s, s.@sum_score) 
              END;
   #test

   #Output
 IF file_path != "" THEN
     f.println("Vertex_ID", "Harmonic");
 END;

 IF print_accum THEN
     PRINT @@top_scores_heap AS top_scores;
     IF display_edges THEN
         PRINT Start[Start.@sum_score];
 Start = SELECT s
 FROM Start:s -(e_type:e)->:t
 ACCUM @@edge_set += e;
 PRINT @@edge_set;
             END;
 END;
}

CREATE QUERY tg_weighted_random_walk_sub(VERTEX source, INT length, INT num_walks, FLOAT p, FLOAT q, FILE f) {
  /* This query impletemented random walk in Node2vec paper. Link: https://cs.stanford.edu/~jure/pubs/node2vec-kdd16.pdf
  Input: source vertex, random walk length, walk times for each vertex, output file, set of valid edge types
  Output: random walk sequence
  */

  SetAccum<VERTEX> @@pick_set;
  ListAccum<FLOAT> @@prob_list;
  ListAccum<VERTEX> @@candidates_list;
  ListAccum<VERTEX>@@path_list;
  FLOAT Prob;
  SumAccum<INT> @sum_d_tx = 2;
  SumAccum<FLOAT>@sum_score;


  FOREACH i in RANGE[0,num_walks-1] DO
      // add source vertex into path
      @@path_list += source;
      Start = {source};

      WHILE(Start.size()>=0 ) LIMIT length DO
          // Calculate transition probability
          Start = SELECT s
                  FROM Start:s-(:e)-:t
                  POST-ACCUM
                      IF t.@sum_d_tx ==0 THEN
                          t.@sum_score = (1/p) //* e.score
                      ELSE IF t.@sum_d_tx == 1 THEN
                          t.@sum_score = 1 //t.score
                      ELSE
                          t.@sum_score = (1/q) //* e.score
                      END;
          // Reset candidates @d_tx =2, start vertex @d_dx = 0
          // store score and vertex of candidates
          candidates = SELECT t
                       FROM Start:s-(:e)-:t
                       ACCUM t.@sum_d_tx = 2, s.@sum_d_tx = 0
                       POST-ACCUM @@prob_list +=t.@sum_score,
                  @@candidates_list +=t;

          IF candidates.size() == 0 THEN
              @@path_list += source;
              continue;
          END;
          // generate prob value based on weight distribution
          Prob = tg_random_distribution(@@prob_list);
          //print Prob;
          // pick vertex that generated by random function
          @@path_list += @@candidates_list.get(Prob);
          @@pick_set += @@candidates_list.get(Prob);

          //print @@pick;
          // select the vertex that connected with @@pick in candidates and set @d_tx=1
          sel = SELECT s
                FROM candidates:s-(:e)-:t
                WHERE t == @@candidates_list.get(Prob)
                POST-ACCUM s.@sum_d_tx =1;

          // select @@pick as the next start vertex
          Start = @@pick_set;

          // clear accumulators to start next iteration
          @@pick_set.clear();
          @@prob_list.clear();
          @@candidates_list.clear();
      END;  // WHILE
      PRINT @@path_list;
      f.println(@@path_list);
      @@path_list.clear();
  END; // FOREACH
}

CREATE QUERY tg_closeness_cent_approx (
    SET<STRING> v_type, SET<STRING> e_type, STRING re_type, INT top_k=100, INT k = 100,  INT max_hops = 10,  DOUBLE epsilon = 0.1,  BOOL print_accum = true, 
    STRING file_path = "",  INT debug = 0,  INT sample_index = 0,  INT max_size = 1000,   BOOL wf = True ) {
      
    /* Compute Closeness Centrality for each VERTEX. 
    Use multi-sourse BFS.
    Parameters:
    v_type: vertex types to traverse                 print_accum: print JSON output
    e_type: edge types to traverse                   k : sample num
    epsilon : eerror parameter                       debug: debug flag -- 0: No LOG;1: LOG without the sample-node bfs loop;2: ALL LOG.
    re_type: reverse edge type in directed graph, in undirected graph set re_type=e_type
    max_hops: max BFS explore steps ,look only this far from each vertex    
    file_path: file to write CSV output to
    top_k: report only this many top scores          sample_index : random sample group
    max_size : max size of connected components using exact closeness algorithm
    wf: Wasserman and Faust normalization factor for multi-component graphs 
    */ 
    
    TYPEDEF TUPLE<VERTEX Vertex_ID, FLOAT score> Vertex_Score;
    HeapAccum<Vertex_Score>(top_k, score DESC) @@top_scores_heap;
    MinAccum<INT> @min_cc_id;      # each vertex's tentative component id
    MapAccum<INT,INT> @@cc_size_map;    # component size 
    MapAccum<VERTEX,INT> @@cc_map;   # closeness centrality of each node
    SumAccum<INT> @sum_vid;  # internal id
    SumAccum<INT> @sum_deltav;  # the distance from the pivot
    SumAccum<INT> @@sum_curDist,@@sum_dist;  # the distance of BFS algorithm 
    MaxAccum<VERTEX> @max_pivot;  # the pivot of the node
    SetAccum<VERTEX> @@sample_set;  # sample node set
    OrAccum<BOOL> @or_sampled;  # sample node
    OrAccum<BOOL> @or_visited,@or_visited0;  # visited node in BFS
    SumAccum<INT> @@sum_total_dist;  # the sum distance of the sample node
    SumAccum<INT> @@sum_nonsample;  # the size of non-sample node in the visiting node
    MapAccum<VERTEX,INT> @@cdist_map;  # the shortest distance between the sample node
    AvgAccum @avg_lavg; # LAVG: the average distance from the sample node within the threshold radius
    SumAccum<INT> @sum_hcsum; # HCSUM: the sum distance from the sample node outside the threshold radius
    SumAccum<INT> @sum_sdist; # record the distance when the sample node cannot be decided
    SumAccum<INT> @sum_lnum; # number of nodes within the threshold radius
    SumAccum<INT> @sum_hsum; # HSUM: the sum distance from non sample node outside the threshold radius
    SumAccum<FLOAT> @sum_total_dist; # the sum distance of the nodes
    SumAccum<FLOAT> @sum_closeness; # the closeness centrality
    SumAccum<INT> @@sum_count = 1;#used to set unique ID
    SumAccum<INT> @sum_id; #store the unique ID
    SetAccum<INT> @@a_set; #used to set unique ID
    MapAccum<INT,INT> @@map; #used to set unique ID 
    BitwiseOrAccum @bitwise_or_seen;
    BitwiseOrAccum @bitwise_or_visit; 
    BitwiseOrAccum @bitwise_or_visit_next; #use bitwise instead of setAccum
    SumAccum<INT> @@sum_curr_dist; #current distance
    SumAccum<INT> @sum_res; #Result, sum of distance
    SumAccum<INT> @sum_size; #get graph size
    FILE f(file_path);
    INT batch_num;
    INT num_vert,partition,i;
    datetime t1,t2;
    FLOAT eps = 0.00001;
  
    Start = {v_type};
    LOG(debug > 0,"query start",Start.size());
    
    # Total number of nodes
    num_vert = Start.size();
  
    # Initialize: Label each vertex with its own internal ID
    comp = SELECT x 
           FROM Start:x
     POST-ACCUM x.@min_cc_id = getvid(x);

    # Propagate smaller internal IDs until no more ID changes can be DOne
    WHILE (comp.size()>0) DO
        comp = SELECT t 
               FROM comp:s-(e_type)->v_type:t
         ACCUM t.@min_cc_id += s.@min_cc_id   // If s has a smaller id than t, copy the id to t
         HAVING t.@min_cc_id != t.@min_cc_id'; 
    END;
  
    LOG(debug > 0,"find connected components");
    
    # get components size
    Start = SELECT s 
            FROM Start:s
      POST-ACCUM @@cc_size_map += (s.@min_cc_id->1);
    LOG(debug > 0,"size of connected components",@@cc_size_map.size());
  
    FOREACH (cc_id,cc_num) IN @@cc_size_map DO        
        IF cc_num > max_size THEN
  # for large components, get estimate closeness centrality for each vertex
  # partition size
            Conn = SELECT s 
                   FROM Start:s
       WHERE s.@min_cc_id == cc_id;   
 
       IF cc_num % k == 0 THEN
           partition = cc_num/k;
       ELSE
           partition = cc_num/k + 1;
       END;
   print partition;
         PRINT cc_num;
         
   # sampling
   snode = SELECT s 
                FROM Conn:s
          ACCUM s.@sum_vid += getvid(s)
    HAVING s.@sum_vid % partition== sample_index
    LIMIT k;
   i = sample_index;
        
   WHILE snode.size() < k DO
       i = (i + 1) % partition;
       snode = SELECT s 
             FROM Conn:s
       HAVING s.@sum_vid % partition == i
       LIMIT k;
   END;
        
   snode = SELECT s 
                 FROM snode:s
     ACCUM s.@or_sampled += True,
           @@sample_set += s;
   LOG(debug > 0, "sampling finished", snode.size());
   PRINT snode.size();    
        
   # Set all sample node as the starting point
         @@sum_curDist = 0;
        src = SELECT s 
              FROM snode:s
        ACCUM s.@max_pivot += s,
        s.@sum_deltav += @@sum_curDist,
                    s.@or_visited0 += True;
   LOG(debug > 0, "initialize pivot query", src.size());
    
   # BFS: get the pivot of all nodes and its distance from the pivot
   WHILE src.size() > 0 DO
       @@sum_curDist += 1;
       src = SELECT t 
                  FROM src:s-(e_type)->v_type:t
      WHERE t.@or_visited0 == False
      ACCUM t.@max_pivot += s.@max_pivot
                  POST-ACCUM t.@or_visited0 += True,
           t.@sum_deltav = @@sum_curDist;
   END;
   LOG(debug > 0, "pivot query finished", @@sum_curDist);
    
   # get closeness for community which is larger than maxsize
   FOREACH sample_node IN @@sample_set DO
   # set one sample node as starting node
            LOG(debug > 1,"one sample_node begin");
      vnode = {sample_node};
      vnode = SELECT s 
                    FROM vnode:s
        ACCUM s.@or_visited += True;
      
      # the pivot of which is the sample node      
      pivot = SELECT s 
                    FROM Conn:s
        WHERE s.@or_sampled == False AND s.@max_pivot == sample_node;
      
      # initialize the shortest distance
      @@sum_dist = 0;
      # initialize the sum of the shortest distance from the sample node
      @@sum_total_dist = 0;
      # initialize the map of shortest distance from other sample node
      @@cdist_map.clear();
      
      WHILE vnode.size() > 0 do
          @@sum_dist += 1;
  
          # initialize the size of non-sample node in the visiting node
          @@sum_nonsample = 0;
      
    # decide whether the sample node is within or outside the threshold radius from the pivot of the visiting node
    vnode = SELECT t 
                        FROM vnode:s-(e_type)->v_type:t
      WHERE t.@or_visited == False
      POST-ACCUM t.@or_visited += True,
           IF t.@or_sampled == True THEN
               @@cdist_map += (t->@@sum_dist)  # record the distance between the sample node
           ELSE
               # the size of non-sample nodes
               @@sum_nonsample += 1,
               # the visiting node in L(u)
               CASE WHEN t.@sum_deltav > epsilon * @@sum_dist / (1 - epsilon) - eps THEN
                   t.@avg_lavg += @@sum_dist
               # the visiting node in HC(u)
               WHEN t.@sum_deltav < epsilon * @@sum_dist /(1 + epsilon) THEN
                   t.@sum_hcsum += @@sum_dist
               # if cannot decided, record the distance
            ELSE
               t.@sum_sdist = @@sum_dist
            END
            END;
           # sum distance of the sample node
     @@sum_total_dist += vnode.size() * @@sum_dist;
        
     # decide whether the visiting node is within or outside the threshold radius from the pivot of which is the sample node
     pivot = SELECT s 
                         FROM pivot:s
       POST-ACCUM
           IF s.@sum_deltav > epsilon * @@sum_dist - eps THEN
               s.@sum_lnum += vnode.size()   # the node num within the threshold radius
           ELSE
         s.@sum_hsum += @@sum_nonsample * @@sum_dist   # the sum distance of non-sample node outside the threshold radius
           END;
             END;
      
      sampleNode = {sample_node};
      
      # the sum distance of the sample node
      sampleNode = SELECT s 
                               FROM sampleNode:s
                   ACCUM s.@sum_total_dist += @@sum_total_dist;      
      
      # calculate the node which has not been decided before
      post_set = SELECT s 
                         FROM Conn:s
       WHERE s.@sum_sdist > 0
       POST-ACCUM
           IF @@cdist_map.get(s.@max_pivot) * epsilon < s.@sum_deltav + eps THEN
               s.@avg_lavg += s.@sum_sdist    # within the threshold radius
           ELSE
         s.@sum_hcsum += s.@sum_sdist
           END;
              
       # clear the BFS visited accumulator and the distance recorded
       Conn = SELECT s 
                          FROM Conn:s
        POST-ACCUM s.@or_visited = False, s.@sum_sdist = 0;
                   LOG(debug > 1,"one sample_node end");
  
  
        END;
        
        LOG(debug > 0, "BFS from sample node finished");
        #END;
              #IF cc_num>maxsize THEN 
              Conn = SELECT s 
                   FROM Conn:s
       ACCUM
           IF s.@or_sampled == False THEN
               s.@sum_total_dist = s.@avg_lavg * s.@sum_lnum + s.@sum_hcsum + s.@sum_hsum
           END
       POST-ACCUM 
                       IF s.@sum_total_dist>0 Then
               IF wf == True THEN
             s.@sum_closeness = ((cc_num-1) * 1.0 / (num_vert-1)) * ((cc_num-1) * 1.0 /s.@sum_total_dist)
         ELSE
             s.@sum_closeness = ((cc_num - 1) * 1.0) / (s.@sum_total_dist * 1.0)
         END
                       END,
           
           IF print_accum THEN 
                           @@top_scores_heap += Vertex_Score(s, s.@sum_closeness) 
                       END,
  
           IF file_path != "" THEN 
                           f.println(s, s.@sum_closeness) 
                       END;
        
         # calculate closeness estimation for all nodes
         LOG(debug > 0, "closeness_est end", Start.size());
        
               #clear the sample set
         @@sample_set.clear();
         END;
    END;
    LOG(debug > 0,"closeness finished");
  
    all = SELECT s 
          FROM Start:s 
          WHERE @@cc_size_map.get(s.@min_cc_id)<max_size;
    num_vert = all.size();
    batch_num = num_vert/62;
    IF batch_num==0 THEN 
        batch_num=1;
    END;
  
    #Calculate the sum of distance to other vertex for each vertex
    FOREACH index IN RANGE[0, batch_num-1] DO
        Start = SELECT s 
                FROM all:s
                WHERE getvid(s)%batch_num == index
                POST-ACCUM @@map+=(getvid(s)->0),@@a_set+=getvid(s);
  
        FOREACH ver IN @@a_set DO 
            @@map+=(ver->@@sum_count); @@sum_count+=1;
        END; #set a unique ID for each vertex, ID from 1-63
  
        Start = SELECT s 
                FROM Start:s 
                POST-ACCUM s.@sum_id=@@map.get(getvid(s));
        Start = Select s 
                FROM Start:s
                POST-ACCUM s.@bitwise_or_seen=1<<s.@sum_id,s.@bitwise_or_visit=1<<s.@sum_id; # set initial seen and visit
      @@a_set.clear();
      @@map.clear();
      @@sum_count=0;
      WHILE (Start.size() > 0) LIMIT max_hops DO
            @@sum_curr_dist+=1;
            Start = SELECT t FROM Start:s -(re_type:e)-v_type:t
                    WHERE s.@bitwise_or_visit&-t.@bitwise_or_seen-1>0 AND s!=t #use -t.@seen-1 to get the trverse of t.@seen
                    ACCUM
                      INT c = s.@bitwise_or_visit&-t.@bitwise_or_seen-1,
                      IF c>0 THEN
                          t.@bitwise_or_visit_next+=c,
                          t.@bitwise_or_seen+=c
                      END
                    POST-ACCUM
                        t.@bitwise_or_visit=t.@bitwise_or_visit_next,
                        INT r = t.@bitwise_or_visit_next,
                        WHILE r>0 DO 
                          r=r&(r-1),t.@sum_res+=@@sum_curr_dist,t.@sum_size+=1 #count how many 1 in the number, same as setAccum,size()
                        END,
                        t.@bitwise_or_visit_next=0;
                     END;
        @@sum_curr_dist=0;
        Start = SELECT s 
          FROM all:s 
                POST-ACCUM s.@bitwise_or_seen=0,
               s.@bitwise_or_visit=0;
      END;
      Start = SELECT s 
              FROM all:s
              # Calculate Closeness Centrality for each vertex
              WHERE s.@sum_res>0
        POST-ACCUM 
                  IF wf THEN 
          s.@sum_closeness = (s.@sum_size/(num_vert-1))*(s.@sum_size/s.@sum_res) 
      ELSE 
          s.@sum_closeness = s.@sum_size/s.@sum_res 
      END;    
       #output
       Start = SELECT s 
               FROM Start:s
         POST-ACCUM
                   IF file_path != "" THEN
           f.println(s,s.@sum_closeness)
                   END;
  
       IF print_accum THEN
           PRINT Start[Start.@sum_closeness];
       END;
       LOG(debug > 0,"closeness output finished");
}


CREATE QUERY tg_bfs(SET<STRING> v_type, SET<STRING> e_type,INT max_hops=10, VERTEX v_start,
BOOL print_accum = True, STRING result_attr = "",STRING file_path = "", BOOL display_edges = TRUE){ 
/*Breadth-First Search Algorithm from a single source node
Parameters:
v_type: vertex types to traverse                 v_start : source vertex for traverse
e_type: edge types to traverse                   print_accum: print JSON output
max_hops: look only this far from each vertex    result_attr: INT attr to store results to
file_path: file to write CSV output to           display_edges: output edges for visualization
*/ 

    SumAccum<INT> @@sum_cur_step;  # current step
    SumAccum<INT> @sum_step;  # step from source
    OrAccum @or_visited;
    SetAccum<EDGE> @@edge_set;
    FILE f (file_path);
    Start = {v_start};

    # initialize the step
    @@sum_cur_step = 0;

    # start from the source node
    Start = SELECT s 
            FROM Start:s
            POST-ACCUM s.@or_visited += TRUE;

    # breadth-first search from source
    WHILE Start.size() > 0 LIMIT max_hops DO
        @@sum_cur_step += 1;
        Start = SELECT t 
                FROM Start:s-(e_type:e)-> :t
                WHERE t.@or_visited == FALSE
                ACCUM 
                    IF display_edges THEN 
                        @@edge_set += e 
                    END
                POST-ACCUM 
                    t.@or_visited += TRUE,
                    t.@sum_step = @@sum_cur_step;
    END;
    
    Start = {v_type};
    Start = SELECT s 
            FROM Start:s
            WHERE s.@or_visited == TRUE
            POST-ACCUM 
                IF result_attr != "" THEN 
                    s.setAttr(result_attr, s.@sum_step) 
                END,
                IF file_path != "" THEN 
                    f.println(s, s.@sum_step) 
                END; 
    # output
    IF print_accum THEN
        PRINT Start[Start.@sum_step];
        IF display_edges THEN
            PRINT @@edge_set;
        END;
    END;
}


CREATE QUERY tg_degree_cent(SET<STRING> v_type, SET<STRING> e_type, SET<STRING> re_type, BOOL in_degree = TRUE, BOOL out_degree = TRUE,
  INT top_k=100, BOOL print_accum = TRUE, STRING result_attr = "",STRING file_path = "") {
  /* Compute degree Centrality for each VERTEX.
  Parameters:
  v_type: vertex types to traverse
  e_type: edge types to traverse
  re_type: for indegree use
  in_degree: if you want to count incoming relationships, set it to TRUE
  out_degree: if you want to count outcoming relationships, set it to TRUE
  top_k: report only this many top scores          print_accum: weather print the result
  result_attr: attribute to write result to        file_path: file to write CSV output to
  for undirected graph, you only need to set e_type and indegree
   */
  TYPEDEF TUPLE<VERTEX Vertex_ID, FLOAT score> Vertex_Score;
  HeapAccum<Vertex_Score>(top_k, score DESC) @@top_scores_heap;
  SumAccum<INT> @sum_degree_score;
  FILE f (file_path);

  all = {v_type};
  sll = SELECT s 
        FROM all:s
        ACCUM IF in_degree THEN
                 FOREACH edge_type in re_type DO
                     s.@sum_degree_score+=s.outdegree(edge_type)
                 END
              END,
              IF out_degree THEN
                  FOREACH edge_type in e_type DO
                      s.@sum_degree_score+=s.outdegree(edge_type)
                  END
              END;
  #Output
  IF file_path != "" THEN
      f.println("Vertex_ID", "Degree");
  END;

  Start = SELECT s 
          FROM all:s
  POST-ACCUM
      IF result_attr != "" THEN 
                  s.setAttr(result_attr, s.@sum_degree_score) 
              END,
    
      IF print_accum THEN 
                  @@top_scores_heap += Vertex_Score(s, s.@sum_degree_score) 
              END,
    
      IF file_path != "" THEN 
                  f.println(s, s.@sum_degree_score) 
              END;
      
   IF print_accum THEN
       PRINT @@top_scores_heap AS top_scores;
   END;
}


CREATE QUERY tg_maximal_indep_set_random(STRING v_type, STRING e_type, INT max_iter = 100, BOOL print_accum = TRUE, STRING file_path = ""){
    /*
    Maximal Independent Set query only supports one edge type and works only for undirected graphs at the moment (8/12/20).
    */

    AndAccum @and_active;
    OrAccum @or_selected;
    MinAccum<INT> @min_r_min, @min_r;
    FILE f(file_path);
    INT iter = 0;

    Start = {v_type.*};
    Start = SELECT s
            FROM Start:s
            ACCUM
                IF s.outdegree(e_type) == 0 THEN
                    s.@or_selected += TRUE,
                    s.@and_active += FALSE
                END
            HAVING s.@and_active;

    WHILE Start.size()>0 LIMIT max_iter DO
        Start = SELECT s
                FROM Start:s
                POST-ACCUM
                    s.@min_r_min = GSQL_INT_MAX,
                    s.@min_r = tg_rand_int(GSQL_INT_MIN, GSQL_INT_MAX);

        TMP = SELECT s
              FROM Start:s-(e_type:e)->v_type:t
              WHERE t.@and_active
              ACCUM
                  s.@min_r_min += t.@min_r;

        TMP = SELECT s
              FROM Start:s
              POST-ACCUM
                  IF s.@min_r < s.@min_r_min THEN
                      s.@or_selected += TRUE,
                      s.@and_active += FALSE
                  END
              HAVING s.@or_selected;

        TMP = SELECT s
              FROM TMP:s-(e_type:e)->v_type:t
              ACCUM
                  t.@and_active += FALSE;

        Start = SELECT s
                FROM Start:s
                WHERE s.@and_active;
    END;

    IF file_path != "" THEN
        f.println("Vertex");
    END;

    Start = {v_type.*};
    Start = SELECT s
            FROM Start:s
            WHERE s.@or_selected
            ACCUM
                IF file_path != "" THEN
                    f.println(s)
                END;

    IF print_accum THEN
        PRINT Start;
    END;
}

CREATE QUERY tg_wcc_small_world(STRING v_type, STRING e_type, UINT threshold = 100000,
  BOOL to_show_cc_count=FALSE) {
  ##
  # This query detects weakly connected components based on the following paper:
  # https://www.osti.gov/servlets/purl/1115145
  ##
  SumAccum<UINT> @sum_indegree;
  SumAccum<UINT> @sum_outdegree;
  SumAccum<INT> @sum_degree_product;
  MinAccum<INT> @min_cc_id;
  OrAccum @or_visited;
  MapAccum<INT, SumAccum<UINT>> @@CC_count_map;
  
  # 1. initialization
  Vertices = {v_type};
  All_Vertices = Vertices;

  # 2. calculate the product of in degree and out degree
  # and filter the vertices which have the product no less than the threshold
  PivotCandidates = SELECT s 
                    FROM Vertices:s
                    POST-ACCUM s.@sum_indegree = s.outdegree(e_type),
                               s.@sum_outdegree = s.outdegree(e_type),
                               s.@sum_degree_product = s.@sum_indegree * s.@sum_outdegree
                    HAVING s.@sum_degree_product >= threshold;
  

  # 3. while PotentialPoviots set is not empty, select a pivot and find its CC
  WHILE PivotCandidates.size() > 0 DO
      # select an initial pivot vertex as the vertex in the graph 
      # that has the largest product of its in degree and out degree
      Vertices = SELECT s 
                 FROM PivotCandidates:s
                 ORDER BY s.@sum_degree_product DESC
                 LIMIT  1;
      Vertices = SELECT s 
                 FROM Vertices:s
                 POST-ACCUM s.@or_visited = TRUE,
                            s.@min_cc_id = getvid(s);
      # with the chosen pivot we use BFS algorithm to find all elements in its connected component
      WHILE Vertices.size() > 0 DO
          Vertices = SELECT t 
                     FROM Vertices:s-(e_type:e)-v_type:t
                     WHERE  t.@or_visited == FALSE
                     ACCUM  t.@min_cc_id = s.@min_cc_id
                     POST-ACCUM t.@or_visited += TRUE;
      
      END;
      # remove the visited vertices from the PivotCandidates set
      PivotCandidates = SELECT s 
                        FROM PivotCandidates:s
                        WHERE  s.@or_visited == FALSE;
  END;

  # 4. take the remaining vertices and pass them all off to coloring
  Vertices = SELECT s 
             FROM All_Vertices:s
             WHERE  s.@or_visited == FALSE
             ACCUM  s.@min_cc_id = getvid(s);
             
  WHILE Vertices.size() > 0 DO
      Vertices = SELECT t 
                 FROM Vertices:s-(e_type:e)-v_type:t
                 WHERE  s.@min_cc_id < t.@min_cc_id
                 ACCUM  t.@min_cc_id += s.@min_cc_id;
  END;

  IF to_show_cc_count THEN
      Vertices = {v_type};
      Vertices = SELECT s 
                 FROM Vertices:s
                 POST-ACCUM @@CC_count_map += (s.@min_cc_id -> 1);
      PRINT @@CC_count_map;
  END;
 }

CREATE QUERY tg_betweenness_cent(SET<STRING> v_type, SET<STRING> e_type, STRING re_type,INT max_hops = 10,
  INT top_k = 100, BOOL print_accum = True, STRING result_attr = "",
  STRING file_path = "", BOOL display_edges = FALSE){ 
  
  /* Compute Betweenness Centrality for each VERTEX. 
  Use multi-sourse BFS.
  Link of the paper: http://www.vldb.org/pvldb/vol8/p449-then.pdf
  Parameters:
  v_type: vertex types to traverse                 print_accum: print JSON output
  e_type: edge types to traverse                   result_attr: INT attr to store results to
  re_type: reverse edge type in directed graph, in undirected graph set re_type=e_type
  max_hops: look only this far from each vertex    file_path: file to write CSV output to
  top_k: report only this many top scores          display_edges: output edges for visualization
   */ 
   
  TYPEDEF TUPLE<VERTEX Vertex_ID, FLOAT score> Vertex_Score; #tuple to store betweenness centrality score
  HeapAccum<Vertex_Score>(top_k, score DESC) @@top_scores_heap; #heap to store top K score
  SumAccum<INT> @@sum_curr_dist; #current distance
  BitwiseOrAccum @bitwise_or_visit_next; #use bitwise instead of setAccum
  BitwiseOrAccum @bitwise_or_seen;
  BitwiseOrAccum @bitwise_or_visit; 
  SumAccum<INT> @@sum_count = 1;#used to set unique ID
  SumAccum<INT> @sum_id; #store the unique ID
  SetAccum<INT> @@batch_set; #used to set unique ID
  MapAccum<INT,INT> @@map; #used to set unique ID 
  SetAccum<EDGE> @@edge_set;
  SumAccum<FLOAT> @sum_delta = 0;
  MapAccum<INT,BitwiseOrAccum> @times_map;
  MapAccum<INT,SumAccum<INT>> @sigma_map;
  
  INT empty=0;
  FILE f (file_path);
  INT num_vert;
  INT batch_number;

# Compute betweenness
  all = {v_type};
  num_vert = all.size();
  batch_number = num_vert/60;
    
  IF batch_number == 0 
      THEN batch_number = 1;
  END;
    
  #Calculate the sum of distance to other vertex for each vertex
  FOREACH i IN RANGE[0, batch_number-1] DO
      Current = SELECT s 
                FROM all:s
                WHERE getvid(s)%batch_number == i
                POST-ACCUM @@map+=(getvid(s)->0),
                           @@batch_set+=getvid(s);
  
      FOREACH ver in @@batch_set DO 
          @@map += (ver->@@sum_count); @@sum_count += 1;
      END; #set a unique ID for each vertex, ID from 1-63
    
      Start = SELECT s 
              FROM Current:s 
              POST-ACCUM 
            s.@sum_id=@@map.get(getvid(s));
    
      Start = SELECT s 
              FROM Current:s
              POST-ACCUM 
            s.@bitwise_or_seen = 1<<s.@sum_id,
                    s.@bitwise_or_visit = s.@bitwise_or_seen,
                    s.@sigma_map += (0->1),
                    s.@times_map += (0->s.@bitwise_or_visit); # set initial seen and visit
      @@batch_set.clear();
      @@map.clear();
      @@sum_count=0;
      
      WHILE (Start.size() > 0) LIMIT max_hops DO
          @@sum_curr_dist+=1;
    
          Start = SELECT t 
                  FROM Start:s -(re_type:e)-v_type:t
                  WHERE s.@bitwise_or_visit&-t.@bitwise_or_seen-1>0 AND s!=t #use -t.@seen-1 to get the trverse of t.@seen
                  ACCUM                               #updatevisitNext
                      INT c = s.@bitwise_or_visit&-t.@bitwise_or_seen-1,
                      IF c>0 THEN 
          t.@bitwise_or_visit_next+=c,
                          t.@bitwise_or_seen+=c
                      END,
                      t.@sigma_map+=(@@sum_curr_dist->s.@sigma_map.get(@@sum_curr_dist-1)) #set sigma based on depth
                  POST-ACCUM 
              t.@bitwise_or_visit=t.@bitwise_or_visit_next,
                      t.@times_map+=(@@sum_curr_dist->t.@bitwise_or_visit),
                      t.@bitwise_or_visit_next=0;
      END;
      @@sum_curr_dist+=-1;
    
      Start = SELECT s 
              FROM all:s 
              WHERE s.@sigma_map.get(@@sum_curr_dist)!=0;
    
      WHILE (Start.size()>0) LIMIT max_hops DO
          @@sum_curr_dist+=-1;
          Start = SELECT t 
                  FROM Start:s -(re_type:e)-> v_type:t
          WHERE t.@times_map.get(@@sum_curr_dist)&s.@times_map.get(@@sum_curr_dist+1)!=0  
          ACCUM 
                      FLOAT currValue=t.@sigma_map.get(@@sum_curr_dist)/(s.@sigma_map.get(@@sum_curr_dist+1)*(1+s.@sum_delta)),
                      INT r=t.@times_map.get(@@sum_curr_dist)&s.@times_map.get(@@sum_curr_dist+1),
                      INT plus=0,
                      WHILE r>0 DO 
                          r=r&(r-1),plus=plus+1 #count how many 1 in the number, same as setAccum,size()
                      END,
                      FLOAT value = currValue*plus/2.0,
                      t.@sum_delta+=value;
          Start = SELECT s 
                  FROM all:s 
                  WHERE s.@sigma_map.get(@@sum_curr_dist)!=0;
      END;
      
      @@sum_curr_dist=0;
      Start = SELECT s 
              FROM all:s 
              POST-ACCUM 
            s.@bitwise_or_seen=0,
                    s.@bitwise_or_visit=0,
                    s.@sigma_map.clear(),
                    s.@times_map.clear();
  END;
  
  #Output
  IF file_path != "" THEN
      f.println("Vertex_ID", "Betweenness");
  END;

  Start = SELECT s 
          FROM all:s
          POST-ACCUM 
      IF result_attr != "" THEN 
                  s.setAttr(result_attr, s.@sum_delta) 
              END,
    
      IF print_accum THEN 
                  @@top_scores_heap += Vertex_Score(s, s.@sum_delta) 
              END,
    
      IF file_path != "" THEN 
                  f.println(s, s.@sum_delta) 
              END;
      
   IF print_accum THEN
       PRINT @@top_scores_heap AS top_scores;
       IF display_edges THEN
           PRINT Start[Start.@sum_delta];
   Start = SELECT s
           FROM Start:s -(e_type:e)->:t
           ACCUM 
                         @@edge_set += e;
           PRINT @@edge_set;
        END;
    END;

}

CREATE QUERY tg_greedy_graph_coloring(SET<STRING> v_type,SET<STRING> e_type,UINT max_colors = 999999,
  BOOL print_color_count = TRUE, BOOL display = TRUE, STRING file_path = ""){
  
  /* Greedy Graph Coloring algorithm
  This is a distributed algorithm for coloring graph on large-scale 
  undirect graphs with one edge type.In every iteration, the vertices 
  send its color to its out-neighbors, and the neighbors get assigned 
  a different color. 
  Conditions:
    -  No two adjacent vertices in the graph should have same color
    -  Until above condition is achieved , keep on finding conflicts 
       and resolve them.
    -  Stop the conflict detection when there are no resolution left.
   Arguments:
   -  v_type: Pass a set of all the vertex types which need to colored
               from the graph.
   -  e_type: Pass a set of all the edge types connecting above mentioned
              edge types.
   -  max_colors: Maximum number of colors allowed to color the entire graph. 
                  Use a large number like 999999 unless there is strict limit 
                  required.
   -  print_color_count:  If set to true, the total colors used to color all vertex 
                          types will be displayed.
   -  display: If set to true, will display all the vertices and associated color.
   -  file_path: If provided, will print results with all vertex Set and color to a file.
  */
  
  TYPEDEF tuple<INT color> test_color;
  AndAccum<BOOL> @@and_is_conflict = true;
  HeapAccum<test_color>(max_colors,color ASC) @color_heap;
  SumAccum<INT> @sum_color_vertex;
  MaxAccum<INT> @@max_counter;
  SetAccum<EDGE> @@edge_set;
  SetAccum<Vertex> @@vertex_set;
  FILE f(file_path);
  
  #initialization - coloring All vertices
  coloring = {v_type};
  coloring = SELECT s 
             FROM coloring:s 
             POST-ACCUM s.@sum_color_vertex+=1,
                        @@max_counter+=s.@sum_color_vertex;
  
  #conflict detection and resolution
  WHILE @@and_is_conflict DO
      start= {v_type};
  
      #clear the color heap for all vertices
      start = SELECT s 
              FROM start:s
              POST-ACCUM s.@color_heap.clear();
  
      #create a color heap for all vertices
      start = SELECT t 
              FROM start:s-(e_type:e)-v_type:t
              ACCUM t.@color_heap+=test_color(s.@sum_color_vertex);
  
      #detect all conflicts
      conflict = SELECT t 
                 FROM start:s-(e_type:e)-v_type:t
                 WHERE s.@sum_color_vertex == t.@sum_color_vertex AND getvid(s)>getvid(t);
  
      #resolve all detected conflicts 
      resolved = SELECT t 
                 FROM conflict:t
                 POST-ACCUM 
                     t.@sum_color_vertex = 1, 
                     FOREACH item in t.@color_heap DO
                        CASE WHEN (item.color == t.@sum_color_vertex) THEN
                            t.@sum_color_vertex += 1,@@max_counter+=t.@sum_color_vertex
                        END,
                        CASE WHEN (item.color > t.@sum_color_vertex) THEN  
                            BREAK 
                        END
                     END;
  
      #ending the conflict detection and resolution
      IF resolved.size()<1 THEN 
          @@and_is_conflict+=false; 
      END;
  
  END;
  
  #collecting results
  start = {v_type};
  start = SELECT t 
          FROM start:s-(e_type:e)-v_type:t
          POST-ACCUM 
              IF file_path != "" THEN
                  f.println(t,t.@sum_color_vertex)
              END;
  
  #printing and storing the results
  IF print_color_count THEN
      PRINT @@max_counter AS color_count;
  END;
    
  IF display THEN
      PRINT start [start.@sum_color_vertex];
  END;

}

CREATE QUERY tg_resource_allocation(VERTEX a, VERTEX b, SET<STRING> e_type, BOOL print_res = TRUE){ 
    /*
    This query calculates the resource allocation value between two vertices.
    Higher the number, the closer two vertices are. A 0 value indicates two vertices are not close.

    Further detail of caluclation is found here: https://arxiv.org/abs/0901.0553

    Parameters :
        a : Input vertex one
        b : Input vertex two
        e_type: edge types to traverse. If all edge types are desired, pass in "ALL" to the set.
        print_res: Boolean of if you want to print result (True by default)
    */
    SumAccum<DOUBLE> @sum_num_neighbors;
    SumAccum<DOUBLE> @@sum_closeness;
    avs = {a};
    bvs = {b};

    # See if user specified edge types to traverse
    IF "ALL" NOT IN e_type THEN
        # Get Neighbors in Common
        na = SELECT n 
             FROM avs -(e_type)-> :n;  # Get vertex A's neighbors 
  
        nb = SELECT n 
             FROM bvs -(e_type)-> :n;  # Get vertex B's neighbors 
  
        u = na INTERSECT nb;  # Get neighbors in common 
        # count number of neighbors of in-common vertices
        tmp = SELECT p 
              FROM u:p -(e_type)- :r 
              ACCUM p.@sum_num_neighbors += 1; 
  
    ELSE  # traverse all edge types
        na = SELECT n 
             FROM avs -()-> :n;  # Get vertex A's neighbors 
  
        nb = SELECT n 
             FROM bvs -()-> :n;  # Get vertex B's neighbors 
  
        u = na INTERSECT nb;  # Get neighbors in common 
        tmp = SELECT p 
              FROM u:p -()- :r 
              ACCUM p.@sum_num_neighbors += 1;  # count number of neighbors of in-common vertices
    END;
    res = SELECT p 
          FROM tmp:p 
          ACCUM @@sum_closeness += 1/p.@sum_num_neighbors;  # calculates closeness measure

    IF print_res THEN 
        PRINT @@sum_closeness;
    END;
}

CREATE QUERY tg_jaccard_nbor_ap_batch ( INT top_k = 10, SET<STRING> v_type, SET<STRING> feat_v_type, SET<STRING> e_type, SET<STRING> re_type, 
STRING similarity_edge, INT src_batch_num = 50, INT nbor_batch_num = 10, BOOL print_accum = true, INT print_limit = 50, STRING file_path = "") {
  /*
    Calculates the Jaccard Neighborhood Similarity between all vertices using a common feature vertex type.
      Jaccard Similarity = intersection_size / (setSize_all + setSize_self - intersection_size)

    Parameters :
      top_k   : # of top scores to report for each vertex
      v_type  : vertex type to compare
      feat_v_type  : feature vertex type
      e_type  : edge type from source vertex to feature vertex type
      re_type  : edge type from feature vertex to source vertex
      similarity_edge : edge type for storing vertex-vertex similarity scores
      src_batch_num  : how many batches to split the source vertices into
      nbor_batch_num : how many batches to split the 2-hop neighbor vertices into
      print_accum : print JSON output
      print_limit : number of source vertices to print, -1 to print all
      file_path : file to write CSV output to
  */

  TYPEDEF TUPLE<VERTEX ver, FLOAT val> Res_Tup; // storing results in the Heap
  MapAccum<VERTEX, INT> @@set_size_map, @intersection_size_map; // set sizes of all vertices
  SetAccum<STRING> @@all_e_types_set;
  SumAccum<FLOAT> @sum_outdegree;
  HeapAccum<Res_Tup>(top_k, val DESC) @sim_heap; // stores topK similarity results
  FILE f (file_path);
  INT print_count;

  all_vertices = {v_type};
  all_vertices = SELECT s 
                 FROM all_vertices:s -(e_type:e)- v_type:t
                 ACCUM 
                     s.@sum_outdegree += 1;

  FOREACH i IN RANGE[0, src_batch_num-1] DO
      // store number of features for each source vertex
      src_batch = SELECT s 
                  FROM all_vertices:s
                  WHERE getvid(s) % src_batch_num == i
                  ACCUM
                      @@set_size_map += (s -> s.@sum_outdegree);

      // store number of source vertices that share common features
      common_features = SELECT t 
                        FROM src_batch:s-(e_type:e)-feat_v_type:t
                        ACCUM t.@intersection_size_map += (s -> 1);

      FOREACH j IN RANGE[0, nbor_batch_num-1] DO
          others = SELECT t 
                   FROM common_features:s-(re_type:e)-v_type:t
                   WHERE getvid(t) % nbor_batch_num == j
                   ACCUM
                       t.@intersection_size_map += s.@intersection_size_map;
          others = SELECT s 
                   FROM others:s
                   ACCUM
                       // perform similarity computation and store results
                       FLOAT div = 0,
                       FOREACH (k,v) IN s.@intersection_size_map DO
                           IF k == s THEN
                               CONTINUE
                           END,
                           div = @@set_size_map.get(k) + s.@sum_outdegree - v,
                           IF div > 0 THEN
                               k.@sim_heap += Res_Tup(s, v/div)
                           END
                       END
                   POST-ACCUM
                       s.@intersection_size_map.clear();
      END;

      IF print_accum == TRUE THEN
          IF print_limit == -1 THEN
              PRINT src_batch[src_batch.@sim_heap];
          ELSE
              IF print_count < print_limit THEN
                  print_batch = SELECT s 
                                FROM src_batch:s 
                                LIMIT print_limit - print_count;
                  print_count = print_count + src_batch.size();
                  PRINT print_batch[print_batch.@sim_heap];
              END;
          END;
      END;

      src_batch = SELECT s 
                  FROM src_batch:s
                  POST-ACCUM
                      FOREACH tup IN s.@sim_heap DO
                          IF file_path != "" THEN
                              f.println(s, tup.ver, tup.val)
                          END,
                          IF similarity_edge != "" THEN
                              INSERT INTO EDGE similarity_edge VALUES (s, tup.ver, tup.val)
                          END
                      END,
                      s.@sim_heap.clear();
                      @@set_size_map.clear();
  END;
}

CREATE QUERY tg_knn_cosine_cv_sub (VERTEX source, SET<STRING> e_type, SET<STRING> re_type, STRING v_label, STRING weight, INT max_k) RETURNS (ListAccum<STRING>) {

  /* This subquery returns a list of predicted label for a source vertex with respect to different k within a given range. 
*/ 
    TYPEDEF TUPLE <label STRING, similarity FLOAT> Label_Score;
    HeapAccum<Label_Score>(max_k, similarity DESC) @@top_labels_heap;  # heap stores the (label, similarity) tuple, order by similarity score
    SumAccum<FLOAT> @sum_numerator, @@sum_norm1, @sum_norm2, @sum_similarity;
    MapAccum<STRING, INT> @@count_map;
    ListAccum<STRING> @@predicted_label_list;  # list of predicted labels to return
    INT max_count = 0;
    STRING predicted_label;   # predicted label in each iteration
    INT k;

    # calculate similarity and find the top k nearest neighbors
    start = {source};
    subjects = SELECT t
               FROM start:s -(e_type:e)-> :t
               ACCUM t.@sum_numerator = e.getAttr(weight, "FLOAT"),
                     @@sum_norm1 += pow(e.getAttr(weight, "FLOAT"), 2);

    neighbours = SELECT t
                 FROM subjects:s -(re_type:e)-> :t
                 WHERE t != source AND t.getAttr(v_label, "STRING") != ""    
 # only consider the neighbors with known label
                 ACCUM t.@sum_numerator += s.@sum_numerator * e.getAttr(weight, "FLOAT");

    kNN = SELECT s
          FROM neighbours:s -(e_type:e)-> :t
          ACCUM s.@sum_norm2 += pow(e.getAttr(weight, "FLOAT"), 2)
          POST-ACCUM @@top_labels_heap += Label_Score(s.getAttr(v_label, "STRING"), s.@sum_numerator/sqrt(@@sum_norm1 * s.@sum_norm2)); 
  # store the label and similarity score in a heap 

    # iterate the heap and calculate label count for different k
    k = 1;
    FOREACH item IN @@top_labels_heap DO  
        @@count_map += (item.label -> 1);   # count is a map, key is the label, value is the count of the label
        IF @@count_map.get(item.label) > max_count THEN
            max_count = @@count_map.get(item.label);
            predicted_label = item.label;
        END;
        @@predicted_label_list += predicted_label;  # list of predicted labels
        k = k+1;
    END;
      
    PRINT @@predicted_label_list;
    RETURN @@predicted_label_list;
}

CREATE QUERY tg_influence_maximization_greedy(STRING v_type,STRING e_type,STRING wt_attr,INT top_k,
  BOOL print_accum = True, STRING file_path = "") {
  /* get the vertex which has maximum influence.
  Parameters:
  v_type: vertex types to traverse
  e_type: edge types to traverse
  wt_attr: enter weight attribute name
  top_k: report only this many top scores
  file_path: file to write CSV output to
  print_accum: weather print the result
   */  

  TYPEDEF TUPLE<VERTEX Vertex_ID, FLOAT score> Vertex_Score;
  HeapAccum<Vertex_Score>(top_k, score DESC) @@top_scores_heap;
  OrAccum @or_in_seed;
  OrAccum @or_influenced;
  SumAccum<FLOAT> @sum_influence_value;
  ListAccum<Vertex_Score> @@res_list;
  VERTEX k;
  FILE f (file_path);
  start = {v_type};

  FOREACH i in RANGE[0,top_k-1] DO
      start = SELECT s
              FROM start:s-(e_type:e)-v_type:v
              WHERE s.@or_in_seed == FALSE AND v.@or_influenced == FALSE
              ACCUM
                  IF wt_attr != ""  THEN
                      s.@sum_influence_value+=e.getAttr(wt_attr,"FLOAT")
                  ELSE
                      s.@sum_influence_value+=1
                  END
              POST-ACCUM
                  @@top_scores_heap+=Vertex_Score(s,s.@sum_influence_value),
                  s.@sum_influence_value=0;

      k= @@top_scores_heap.top().Vertex_ID ;
      @@res_list += @@top_scores_heap.top();
      temp = SELECT s
             FROM start:s-(e_type:e)-v_type:v
             WHERE s==k
             ACCUM
                  v.@or_influenced+=TRUE,s.@or_in_seed+=TRUE;
       @@top_scores_heap.clear();
  END;
  #Output
  IF file_path != "" THEN
      f.println("Vertex_ID", "Vertex Rank");
      FOREACH i IN RANGE[0,@@res_list.size()-1] DO
          f.println(@@res_list.get(i).Vertex_ID,i+1);
      END;
  END;

  IF print_accum THEN
      PRINT @@res_list;
  END;
}

CREATE QUERY tg_max_BFS_depth(VERTEX source, SET<STRING> e_type) RETURNS (INT){ 
OrAccum @or_visited;
INT depth=-1;
start = {source};
WHILE start.size() > 0 DO
    depth = depth + 1;
    start = SELECT t
    FROM start:s -(e_type:e) ->:t 
    WHERE NOT t.@or_visited
    ACCUM t.@or_visited = TRUE;
END;
RETURN depth;
}

CREATE QUERY tg_lcc (STRING v_type, STRING e_type,INT top_k=100,BOOL print_accum = True, STRING result_attr = "",
  STRING file_path = "", BOOL display_edges = FALSE){ 
   /*
   The Local Clustering Coefficient algorithm computes the local clustering coefficient 
   for each node in the graph. 
   lcc = Number_trangles/((n-1)n/2)
   Here n is the outdegreeof vertex.
  Parameters:
  v_type: vertex types to traverse                 print_accum: print JSON output
  e_type: edge types to traverse                   result_attr: INT attr to store results to
  top_k: report only this many top scores          file_path: file to write CSV output to
  display_edges: output edges for visualization
   */
   
  TYPEDEF TUPLE<VERTEX Vertex_ID, FLOAT score> Vertex_Score;
  HeapAccum<Vertex_Score>(top_k, score DESC) @@top_scores_heap;
  SumAccum<FLOAT> @sum_tri; #number of trangles
  SumAccum<FLOAT> @sum_lcc; #lcc value
  SetAccum<int> @neighbors_set; #neighbors set
  OrAccum @or_self_con; #check if the vertex is self-connect
  SetAccum<EDGE> @@edge_set;
  FILE f (file_path);
  # Here we compute the intersection for 2 points on the triangle.
  
  Start = {v_type};
  Start = SELECT s 
          FROM Start:s-(e_type)-v_type:t
          ACCUM 
              IF getvid(s) != getvid(t) THEN 
                  t.@neighbors_set += getvid(s)
              ELSE   
                  t.@or_self_con+=TRUE 
              END;# check id the vertex is self-connect
                           
  Start = SELECT s 
          FROM Start:s-(e_type)-v_type:t
          WHERE s.outdegree(e_type)>1
          ACCUM 
              s.@sum_tri+=COUNT((s.@neighbors_set INTERSECT t.@neighbors_set))
          POST-ACCUM 
              IF s.@or_self_con  AND s.outdegree(e_type)<3 THEN 
                      s.@sum_lcc+=0
              ELSE IF s.@or_self_con AND s.outdegree(e_type)>2 THEN 
                      s.@sum_lcc+= (((s.@sum_tri+1-s.outdegree(e_type)))/((s.outdegree(e_type)-2)*(s.outdegree(e_type)-1)))
              ELSE 
                      s.@sum_lcc+= ((s.@sum_tri)/((s.outdegree(e_type)-0)*(s.outdegree(e_type)-1)))
              END;
        
  #output
  Start = SELECT s 
          FROM Start:s
          # Calculate Closeness Centrality for each vertex
  POST-ACCUM 
      IF result_attr != "" THEN 
          s.setAttr(result_attr, s.@sum_lcc) 
              END,
      IF print_accum THEN 
          @@top_scores_heap += Vertex_Score(s, s.@sum_lcc) 
              END,
      IF file_path != "" THEN 
          f.println(s, s.@sum_lcc) 
      END;
      
  IF file_path != "" THEN
      f.println("Vertex_ID", "lcc");
  END;

  IF print_accum THEN
      PRINT @@top_scores_heap AS top_scores;
      IF display_edges THEN
          PRINT Start[Start.@sum_lcc];
  Start = SELECT s
  FROM Start:s -(e_type:e)->:t
  ACCUM @@edge_set += e;
  PRINT @@edge_set;
      END;
  END;
}

CREATE QUERY tg_closeness_cent(SET<STRING> v_type, SET<STRING> e_type, STRING re_type,INT max_hops = 10,
  INT top_k = 100, BOOL wf = TRUE, BOOL print_accum = True, STRING result_attr = "",
  STRING file_path = "", BOOL display_edges = FALSE){ 
  
  /* Compute Closeness Centrality for each VERTEX. 
  Use multi-sourse BFS.
  Link of the paper: http://www.vldb.org/pvldb/vol8/p449-then.pdf
  Parameters:
  v_type: vertex types to traverse                 print_accum: print JSON output
  e_type: edge types to traverse                   result_attr: INT attr to store results to
  re_type: reverse edge type in directed graph, in undirected graph set re_type=e_type
  max_hops: look only this far from each vertex    file_path: file to write CSV output to
  top_k: report only this many top scores          display_edges: output edges for visualization
  wf: Wasserman and Faust normalization factor for multi-component graphs */ 
  
  TYPEDEF TUPLE<VERTEX Vertex_ID, FLOAT score> Vertex_Score; #tuple to store closeness centrality score
  HeapAccum<Vertex_Score>(top_k, score DESC) @@top_scores_heap; #heap to store top K score
  SumAccum<INT> @@sum_curr_dist; #current distance
  BitwiseOrAccum @bitwise_or_visit_next; #use bitwise instead of setAccum
  SumAccum<INT> @sum_res; #Result, sum of distance
  SumAccum<INT> @sum_size; #get graph size
  SumAccum<FLOAT> @sum_score;
  BitwiseOrAccum @bitwise_or_seen;
  BitwiseOrAccum @bitwise_or_visit; 
  SumAccum<INT> @@sum_count=1;#used to set unique ID
  SumAccum<INT> @sum_id; #store the unique ID
  SetAccum<INT> @@batch_set; #used to set unique ID
  MapAccum<INT,INT> @@map; #used to set unique ID 
  SetAccum<EDGE> @@edge_set;
  INT empty=0;
  FILE f (file_path);
  INT num_vert;
  INT batch_number;
# Compute closeness
  all = {v_type};
  
  num_vert = all.size();
  batch_number = num_vert/60;
  IF batch_number==0 THEN batch_number=1; END;
    
  #Calculate the sum of distance to other vertex for each vertex
  FOREACH i IN RANGE[0, batch_number-1] DO
          Start = SELECT s 
                  FROM all:s
                  WHERE getvid(s)%batch_number == i
                  POST-ACCUM 
        @@map+=(getvid(s)->0),
                        @@batch_set+=getvid(s);
  
          FOREACH ver in @@batch_set DO 
              @@map+=(ver->@@sum_count); 
      @@sum_count+=1;
          END; #set a unique ID for each vertex, ID from 1-63
    
          Start = SELECT s 
                  FROM Start:s 
                  POST-ACCUM 
         s.@sum_id=@@map.get(getvid(s));
    
          Start = Select s 
                  FROM Start:s
                  POST-ACCUM 
         s.@bitwise_or_seen=1<<s.@sum_id,
                         s.@bitwise_or_visit=1<<s.@sum_id; # set initial seen and visit s.@seen1 s.@seen2 
          @@batch_set.clear();
          @@map.clear();
          @@sum_count=0;
      
          WHILE (Start.size() > 0) LIMIT max_hops DO
                @@sum_curr_dist+=1;
                Start = SELECT t FROM Start:s -(re_type:e)-v_type:t
                        WHERE s.@bitwise_or_visit&-t.@bitwise_or_seen-1>0 and s!=t #use -t.@seen-1 to get the trverse of t.@seen
                        ACCUM
                              INT c = s.@bitwise_or_visit&-t.@bitwise_or_seen-1,
                              IF c>0 THEN
                                  t.@bitwise_or_visit_next+=c,
                                  t.@bitwise_or_seen+=c
                              END
                        POST-ACCUM
                              t.@bitwise_or_visit=t.@bitwise_or_visit_next,
                              INT r = t.@bitwise_or_visit_next,
                              WHILE r>0 DO 
                                    r=r&(r-1),t.@sum_res+=@@sum_curr_dist,t.@sum_size+=1 #count how many 1 in the number, same as setAccum,size()
                              END,
                              t.@bitwise_or_visit_next=0;
          END;
          @@sum_curr_dist=0;
          Start = SELECT s 
                  FROM all:s 
                  POST-ACCUM 
                        s.@bitwise_or_seen=0,s.@bitwise_or_visit=0;
  END;
  
  #Output
IF file_path != "" THEN
    f.println("Vertex_ID", "Closeness");
END;

  Start = SELECT s 
          FROM all:s
  # Calculate Closeness Centrality for each vertex
          WHERE s.@sum_res>0
          POST-ACCUM 
                IF wf THEN 
                    s.@sum_score = (s.@sum_size*1.0/(num_vert-1))*(s.@sum_size*1.0/s.@sum_res) 
                ELSE 
                    s.@sum_score = s.@sum_size*1.0/s.@sum_res*1.0 
                END,
    
            IF result_attr != "" THEN 
                    s.setAttr(result_attr, s.@sum_score) 
                END,
    
              IF print_accum THEN 
                    @@top_scores_heap += Vertex_Score(s, s.@sum_score) 
                END,
    
            IF file_path != "" THEN 
                    f.println(s, s.@sum_score) 
                END;
   #test

   IF print_accum THEN
       Start = SELECT s 
               FROM all:s
               WHERE s.@sum_res<=0 
               POST-ACCUM 
                     @@top_scores_heap += Vertex_Score(s, -1);
       PRINT @@top_scores_heap AS top_scores;
       IF display_edges THEN
   PRINT Start[Start.@sum_score];
       Start = SELECT s
       FROM Start:s -(e_type:e)->:t
       ACCUM 
                     @@edge_set += e;
       PRINT @@edge_set;
       END;
    END;
}

CREATE QUERY tg_slpa (SET<STRING> v_type, SET<STRING> e_type, FLOAT threshold, INT max_iter, INT output_limit,
BOOL print_accum = TRUE, STRING file_path = ""){

/* The algorithm is an extension of the Label Propagation Algorithm for overlapping community detection.
Link of the paper: http://arxiv.org/pdf/1109.5720
Indicate community membership by assigning each vertex multiple community IDs.
Parameters:
  v_type: vertex types to traverse          print_accum: print JSON output
  e_type: edge types to traverse            attr: INT attr to store results to
  threshold: threshold to drop a label      file_path: file to write CSV output to
  max_iter: number of iterations            output_limit: max #vertices to output (-1 = all)
*/

    ListAccum<INT> @label_list;    # the memory of each vertex
    SumAccum<INT> @sum_send;      # the label sended by speaker rule
    MapAccum<INT, INT> @recv_map;     # <label, numlabels>
    MapAccum<INT, FLOAT> @count_map;    # <label, probability>
    MapAccum<INT, INT> @@comm_sizes_map;    # <communityId, communitysize>
    SetAccum<INT> @community_set,@@nest_set,@@comm_set,@@index_set;   # communityId and nested communityId
    ListAccum<INT> @@community_list;    # all communityId
    SetAccum<INT> @@com1_set,@@com2_set,@@tmp1_set,@@tmp2_set;   # community set
    FILE f (file_path);
    Start = {v_type};

    # Initialization: Assign unique labels to the memory of each vertex
    Start = SELECT s FROM Start:s
                     ACCUM s.@label_list += getvid(s);

    # Evolution: Propagate labels to neighbors according to listener and speaker rules until the max iterations is reached
    WHILE True LIMIT max_iter DO
        # Speaker rule: select a random label from its memory with probability proportional to the occurrence frequency of this label in the memory
        Start = SELECT s
                FROM Start:s
                ACCUM s.@sum_send = s.@label_list.get(tg_rand_int(0,s.@label_list.size()-1));

        # listener rule: select the most popular label
        Start = SELECT s
                FROM Start:s -(e_type:e)-> :t
                ACCUM t.@recv_map += (s.@sum_send -> 1)  # count the occurrences of received labels
                POST-ACCUM
                    INT maxV = 0,
                    INT label = 0,
                    # Iterate over the map to get the label that occurs most often
                    FOREACH (k,v) IN t.@recv_map DO
                        CASE WHEN v > maxV THEN
                            maxV = v,
                            label = k
                        END
                    END,
                    t.@label_list += label,
                    t.@recv_map.clear();
    END;

    # Post-processing: remove nodes label seen with probability less than threshold
    Start = SELECT s
            FROM Start:s
            ACCUM
                FOREACH k in s.@label_list DO
                    s.@count_map += (k->1.0/s.@label_list.size())
                END
            POST-ACCUM
                INT label = 0,
                FLOAT maxV = 0,
                FOREACH (k,v) in s.@count_map DO
                    IF v > threshold THEN
                        s.@community_set += k
                    END,
                    IF v > maxV THEN
                        maxV = v,
                        label = k
                    END
                END,
                IF s.@community_set.size() == 0 THEN
                    s.@community_set += label
                END;

    # Removing nested communities, return the maximal community
    Start = SELECT s
            FROM Start:s
            POST-ACCUM
                FOREACH k in s.@community_set DO
                    @@comm_set += k        # get all labels
                END;
    FOREACH k in @@comm_set DO
        @@community_list += k;
    END;

    FOREACH i in RANGE[0,@@community_list.size()-1] DO
        IF @@nest_set.contains(@@community_list.get(i)) THEN
            CONTINUE;
        END;

        @@com1_set.clear();
        @@index_set.clear();

        Comm0 = SELECT s
                FROM Start:s
                WHERE s.@community_set.contains(@@community_list.get(i))
                POST-ACCUM
                    @@com1_set += getvid(s),
                    FOREACH j in RANGE[i+1,@@community_list.size()-1] DO
                        if s.@community_set.contains(@@community_list.get(j)) THEN
                            @@index_set += j
                        END
                    END;

        FOREACH j in @@index_set DO
            @@com2_set.clear();
            @@tmp1_set.clear();
            @@tmp2_set.clear();

            Comm1 = SELECT s
                    FROM Start:s
                    WHERE s.@community_set.contains(@@community_list.get(j))
                    POST-ACCUM @@com2_set += getvid(s);

            @@tmp1_set = @@com2_set MINUS @@com1_set;
            @@tmp2_set = @@com1_set MINUS @@com2_set;
            IF @@tmp1_set.size() == 0 THEN   # community i is superset of community j
                @@nest_set += @@community_list.get(j);
            END;
            IF @@tmp2_set.size() == 0 THEN  # community i is subset of community j
                @@nest_set += @@community_list.get(i);
                BREAK;
            END;
        END;
    END;

    Start = SELECT s
            FROM Start:s
            POST-ACCUM
                FOREACH k in s.@community_set DO
                    IF @@nest_set.contains(k) THEN   # remove nested community
                        s.@community_set.remove(k)
                    ELSE IF print_accum THEN
                        @@comm_sizes_map += (k -> 1)
                    END
                END;

    # output
    Start =  SELECT s
             FROM Start:s
             POST-ACCUM
                 IF file_path != "" THEN
                     f.println(s, s.@community_set)
                 END
             LIMIT output_limit;

    IF print_accum THEN
        PRINT @@comm_sizes_map;
        PRINT Start[Start.@community_set];
    END;
}

CREATE QUERY tg_adamic_adar(VERTEX a, VERTEX b, SET<STRING> e_type, BOOL print_res = TRUE) { 
    /*
    This query calculates the Adamic Adar value between two vertices.
    Higher the number, the closer two vertices are. A 0 value indicates two vertices are not close.

    Formula of caluclation is found here: https://en.wikipedia.org/wiki/Adamic/Adar_index

    Parameters :
        a : Input vertex one
        b : Input vertex two
        e_type: edge types to traverse. If all edge types are desired, pass in "ALL" to the set.
        print_res: Boolean of if you want to print result (True by default)
    */
    SumAccum<INT> @sum_num_neighbors;
    SumAccum<DOUBLE> @@sum_closeness;
    avs = {a};
    bvs = {b};

    # See if user specified edge types to traverse
    IF "ALL" NOT IN e_type THEN
        na = SELECT n 
             FROM avs -(e_type)-> :n;  # Get vertex A's neighbors
             
        nb = SELECT n 
             FROM bvs -(e_type)-> :n;  # Get vertex B's neighbors
             
        u = na INTERSECT nb;  # Get neighbors in common
        tmp = SELECT p 
              FROM u:p -(e_type)- :r 
              ACCUM p.@sum_num_neighbors += 1;  # count number of neighbors of in-common vertices
              
    ELSE  # Traverse all edge types
        na = SELECT n 
             FROM avs -()-> :n;  # Get vertex A's neighbors
             
        nb = SELECT n 
             FROM bvs -()-> :n;  # Get vertex B's neighbors
             
        u = na INTERSECT nb;  # Get neighbors in common
        tmp = SELECT p 
              FROM u:p -()- :r 
              ACCUM p.@sum_num_neighbors += 1;  # count number of neighbors of in-common vertices
    END;
    
    res = SELECT p 
          FROM tmp:p 
          ACCUM @@sum_closeness += 1/log10(p.@sum_num_neighbors);  # calculates closeness measure

    IF print_res THEN 
        PRINT @@sum_closeness; 
    END;
}

CREATE QUERY tg_cosine_nbor_ss (VERTEX source, SET<STRING> e_type, SET<STRING> re_type, STRING weight, INT top_k, INT output_limit,
  BOOL print_accum = TRUE, STRING file_path = "", STRING similarity_edge = "") RETURNS (MapAccum<VERTEX, FLOAT>) {

/* This query calculates the Cosine Similarity between a given vertex and every other vertex.
Cosine similarity = A \dot B / ||A|| \dot ||B||
*/
  SumAccum<FLOAT> @sum_numerator, @@sum_norm1, @sum_norm2, @sum_similarity;
  MapAccum<VERTEX, FLOAT> @@top_k_result_map;
  FILE f (file_path);


  start = {source};
  subjects = SELECT t
             FROM start:s -(e_type:e)-> :t
             ACCUM t.@sum_numerator = e.getAttr(weight, "FLOAT"),
                   @@sum_norm1 += pow(e.getAttr(weight, "FLOAT"), 2);

  neighbours = SELECT t
               FROM subjects:s -(re_type:e)->:t
               WHERE t != source
               ACCUM t.@sum_numerator += s.@sum_numerator * e.getAttr(weight, "FLOAT");

  neighbours = SELECT s
               FROM neighbours:s -(e_type:e)-> :t
               ACCUM s.@sum_norm2 += pow(e.getAttr(weight, "FLOAT"), 2)
               POST-ACCUM s.@sum_similarity = s.@sum_numerator/sqrt(@@sum_norm1 * s.@sum_norm2)
               ORDER BY s.@sum_similarity DESC
               LIMIT top_k;

  neighbours = SELECT s
               FROM neighbours:s
               POST-ACCUM
                 IF similarity_edge != "" THEN
                     INSERT INTO EDGE similarity_edge VALUES(source, s, s.@sum_similarity)
                 END,

                 IF file_path != "" THEN
                     f.println(source, s, s.@sum_similarity)
                 END,
                 IF print_accum THEN
                     @@top_k_result_map += (s -> s.@sum_similarity)
                 END
               LIMIT output_limit;

  IF print_accum THEN
      PRINT neighbours[neighbours.@sum_similarity];
  END;

  RETURN @@top_k_result_map;
}

CREATE QUERY tg_cosine_nbor_ap_batch(STRING vertex_type, STRING edge_type, STRING edge_attribute, INT top_k, BOOL print_accum = true, STRING file_path, STRING similarity_edge, INT num_of_batches = 1) {
  /*
    This query calculates the Cosine Similarity of a given vertex and every other vertex.
      Cosine Similarity = A \dot B/ ||A|| \dot ||B||/

    Parameters :
      vertex_type : start vertex types
      edge_type   : edge type to traverse
      edge_attribute  : name of the attribute on the edge_type
      top_k        : # top scores to report
      print_accum : print JSON output
      file_path   : file to write CSV output to
      similarity_edge : edge type for storing vertex-vertex similarity scores
      num_of_batches  : how many batches to split the query into (trade off parallelism for memory optimization)
  */
  TYPEDEF TUPLE<VERTEX ver, FLOAT val> res_tup; # storing results in the heap
  MapAccum<VERTEX, FLOAT> @numerator_map, @@norm_map; # weight value from edge, normalized value
  HeapAccum<res_tup>(top_k, val desc) @heap; # stores topK similarity results
  FILE f (file_path);

# get numerator value and normalized value from edge attribute
  start = {vertex_type};
  subjects = SELECT t
             FROM start:s -(edge_type:e)-> :t
             WHERE e.getAttr(edge_attribute, "FLOAT") > 0
             ACCUM t.@numerator_map += (s -> e.getAttr(edge_attribute, "FLOAT")),
                   @@norm_map += (s -> pow(e.getAttr(edge_attribute, "FLOAT"), 2));

# compute the cosine simliarity, broken up into 2 parts
  FOREACH i IN RANGE[0, num_of_batches-1] DO
      neighbours = SELECT t
                   FROM subjects:s -(edge_type:e)-> :t
                   WHERE e.getAttr(edge_attribute, "FLOAT") > 0
                   AND getvid(t) % num_of_batches == i
                   ACCUM
                       FOREACH (k,v) IN s.@numerator_map DO
                           CASE WHEN getvid(k) != getvid(t) THEN # for testing purposes, using !=. Use > comparison operation normally
                               t.@numerator_map += (k -> v * e.getAttr(edge_attribute, "FLOAT"))
                           END
                       END;

# get final similarity value and store into heap
    neighbours = SELECT t
                 FROM neighbours:t
                 POST-ACCUM
                     FOREACH (ver, w) IN t.@numerator_map DO
                         CASE WHEN ver != t THEN
                             FLOAT divisor = sqrt(@@norm_map.get(t) * @@norm_map.get(ver)),
                             CASE WHEN divisor == 0 THEN
                                 CONTINUE
                             END,
                             FLOAT sim = w/divisor,
                             t.@heap += res_tup(ver, sim)
                         END
                     END,
                     t.@numerator_map.clear();
  END;

# output to file/JSON or insert edge
  start = SELECT t
          FROM start:t
          POST-ACCUM
              FOREACH tup IN t.@heap DO
                  CASE WHEN tup.val > 0 THEN
                      IF file_path != "" THEN
                          f.println(t, tup.ver, tup.val)
                      END,
                      IF similarity_edge != "" THEN
                          INSERT INTO EDGE similarity_edge VALUES (t, tup.ver, tup.val)
                      END
                  END
              END
          ORDER BY getvid(t) ASC;

  IF print_accum THEN
      PRINT start[start.@heap];
  END;
}

CREATE QUERY tg_knn_cosine_all_sub (VERTEX source, SET<STRING> e_type, SET<STRING> re_type, STRING weight, STRING label, INT top_k) RETURNS (STRING) {

/* This subquery is k-nearest neighbors based on Cosine Similarity between a given vertex and every other vertex.
Cosine similarity = A \dot B / ||A|| \dot ||B||
*/
    SumAccum<FLOAT> @sum_numerator, @@sum_norm1, @sum_norm2, @sum_similarity;
    MapAccum<STRING, INT> @@count_map;
    INT max_count = 0;
    STRING predicted_label;

    # calculate similarity and find the top k nearest neighbors
    start = {source};
    subjects = SELECT t
               FROM start:s -(e_type:e)-> :t
               ACCUM t.@sum_numerator = e.getAttr(weight, "FLOAT"),
                     @@sum_norm1 += pow(e.getAttr(weight, "FLOAT"), 2);

    neighbours = SELECT t
                 FROM subjects:s -(re_type:e)-> :t
                 WHERE t != source AND t.getAttr(label, "STRING") != ""    # only consider the ones with known label
                 ACCUM t.@sum_numerator += s.@sum_numerator * e.getAttr(weight, "FLOAT");

    kNN = SELECT s
          FROM neighbours:s -(e_type:e)-> :t
          ACCUM s.@sum_norm2 += pow(e.getAttr(weight, "FLOAT"), 2)
          POST-ACCUM s.@sum_similarity = s.@sum_numerator/sqrt(@@sum_norm1 * s.@sum_norm2)
          ORDER BY s.@sum_similarity DESC
          LIMIT top_k;

    #predict label
    kNN = SELECT s
          FROM kNN:s
          ACCUM @@count_map += (s.getAttr(label, "STRING") -> 1);

    FOREACH (pred_label, cnt) IN @@count_map DO
        IF cnt > max_count THEN
            max_count = cnt;
            predicted_label = pred_label;
        END;
    END;

    PRINT predicted_label;
    RETURN predicted_label;

}

CREATE QUERY tg_total_neighbors(VERTEX a, VERTEX b, SET<STRING> e_type, BOOL print_res = TRUE) { 
    /*
    This query calculates the number of total neighbors of two vertices.
    Higher the number, the closer two vertices are.

    Parameters :
        a : Input vertex one
        b : Input vertex two
        e_type: edge types to traverse. If all edge types are desired, pass in "ALL" to the set.
        print_res: Boolean of if you want to print result (True by default)
    */

    avs = {a};
    bvs = {b};

    IF "ALL" NOT IN e_type THEN  # Specific edge types defined as parameters
        na = SELECT n 
             FROM avs -(e_type)-> :n;  # Get vertex A's neighbors
  
        nb = SELECT n 
             FROM bvs -(e_type)-> :n;  # Get vertex B's neighbors
  
    ELSE  # Use all edge types
        na = SELECT n 
             FROM avs -()-> :n;  # Get vertex A's neighbors
  
        nb = SELECT n 
             FROM bvs -()-> :n;  # Get vertex B's neighbors
    END;
    u = na UNION nb;  # Get all neighbors
    IF print_res THEN
        PRINT u.size() as closeness; 
    END;
}

CREATE QUERY tg_same_community(VERTEX a, VERTEX b, STRING community_attribute, STRING community_attr_type, BOOL print_res = TRUE) { 
    /*
    This query returns 1 if the two vertices are in the same community, and 0 otherwise.
    Assumes that a community dedection algorithm has already
    been run and the results are stored in an integer vertex attribute.

    Parameters :
        a : Input vertex one
        b : Input vertex two
        communityAttribute: the attribute community information is stored in the graph
        communityAttrType: the type of the attribute that stores community info (e.g. "STRING", "INT")
        print_res: Boolean of if you want to print result (True by default)
    */
    INT aCommunity;
    INT bCommunity;
    avs = {a};
    bvs = {b};

    IF community_attr_type NOT IN ("UINT","INT", "FLOAT", "DOUBLE", "STRING") THEN
        PRINT "communityAttrType not valid option" as errMsg;
        RETURN;
    END;
    
    res = SELECT av
          FROM avs:av 
          POST-ACCUM CASE  community_attr_type 
                     WHEN "UINT" THEN aCommunity = av.getAttr(community_attribute, "UINT")
                     WHEN "INT" THEN aCommunity = av.getAttr(community_attribute, "INT")
                     WHEN "FLOAT" THEN aCommunity = av.getAttr(community_attribute, "FLOAT")
                     WHEN "DOUBLE" THEN aCommunity = av.getAttr(community_attribute, "DOUBLE")
                     WHEN "STRING" THEN aCommunity = str_to_int(av.getAttr(community_attribute, "STRING"))
                     END;
          
    res = SELECT bv
          FROM bvs:bv
          POST-ACCUM CASE  community_attr_type 
                     WHEN "UINT" THEN bCommunity = bv.getAttr(community_attribute, "UINT")
                     WHEN "INT" THEN bCommunity = bv.getAttr(community_attribute, "INT")
                     WHEN "FLOAT" THEN bCommunity = bv.getAttr(community_attribute, "FLOAT")
                     WHEN "DOUBLE" THEN bCommunity = bv.getAttr(community_attribute, "DOUBLE")
                     WHEN "STRING" THEN bCommunity = str_to_int(bv.getAttr(community_attribute, "STRING"))
                     END;
          
    # Check if in same community
    IF aCommunity == bCommunity THEN 
        IF print_res THEN
            PRINT 1;
        END;
    ELSE # Not in same community
        IF print_res THEN
            PRINT 0;
        END;
    END;
}

CREATE QUERY tg_pagerank (STRING v_type, STRING e_type,
 FLOAT max_change=0.001, INT max_iter=25, FLOAT damping=0.85, INT top_k = 100,
 BOOL print_accum = TRUE, STRING result_attr =  "", STRING file_path = "",
 BOOL display_edges = FALSE) {
/*
 Compute the pageRank score for each vertex in the GRAPH
 In each iteration, compute a score for each vertex:
     score = (1-damping) + damping*sum(received scores FROM its neighbors).
 The pageRank algorithm stops when either of the following is true:
 a) it reaches max_iter iterations;
 b) the max score change for any vertex compared to the last iteration <= max_change.
 v_type: vertex types to traverse          print_accum: print JSON output
 e_type: edge types to traverse            result_attr: INT attr to store results to
 max_iter; max #iterations                 file_path: file to write CSV output to
 top_k: #top scores to output              display_edges: output edges for visualization
 max_change: max allowed change between iterations to achieve convergence
 damping: importance of traversal vs. random teleport

 This query supports only taking in a single edge for the time being (8/13/2020).
*/
TYPEDEF TUPLE<VERTEX Vertex_ID, FLOAT score> Vertex_Score;
HeapAccum<Vertex_Score>(top_k, score DESC) @@top_scores_heap;
MaxAccum<FLOAT> @@max_diff = 9999;    # max score change in an iteration
SumAccum<FLOAT> @sum_recvd_score = 0; # sum of scores each vertex receives FROM neighbors
SumAccum<FLOAT> @sum_score = 1;           # initial score for every vertex is 1.
SetAccum<EDGE> @@edge_set;             # list of all edges, if display is needed
FILE f (file_path);

# PageRank iterations
Start = {v_type};                     # Start with all vertices of specified type(s)
WHILE @@max_diff > max_change 
    LIMIT max_iter DO
        @@max_diff = 0;
    V = SELECT s
FROM Start:s -(e_type:e)-> v_type:t
ACCUM 
            t.@sum_recvd_score += s.@sum_score/(s.outdegree(e_type)) 
POST-ACCUM 
            s.@sum_score = (1.0-damping) + damping * s.@sum_recvd_score,
    s.@sum_recvd_score = 0,
    @@max_diff += abs(s.@sum_score - s.@sum_score');
END; # END WHILE loop

# Output
IF file_path != "" THEN
    f.println("Vertex_ID", "PageRank");
END;
V = SELECT s 
    FROM Start:s
    POST-ACCUM 
        IF result_attr != "" THEN 
            s.setAttr(result_attr, s.@sum_score) 
        END,
   
IF file_path != "" THEN 
            f.println(s, s.@sum_score) 
        END,
   
IF print_accum THEN 
            @@top_scores_heap += Vertex_Score(s, s.@sum_score) 
        END;

IF print_accum THEN
    PRINT @@top_scores_heap;
    IF display_edges THEN
        PRINT Start[Start.@sum_score];
Start = SELECT s
        FROM Start:s -(e_type:e)-> v_type:t
        ACCUM @@edge_set += e;
        PRINT @@edge_set;
    END;
END;
}

CREATE QUERY tg_estimate_diameter(SET<STRING> v_type, SET<STRING> e_type, INT seed_set_length, BOOL print_accum = TRUE, STRING file_path = "", BOOL display = FALSE){
  
  MaxAccum<INT> @@max_diameter;
  FILE f (file_path);
  start = {v_type};
  start = SELECT s
          FROM start:s
          LIMIT seed_set_length;
  
  IF display THEN
      PRINT start;
  END;
  
  start = SELECT s
          FROM start:s
          ACCUM @@max_diameter += tg_max_BFS_depth(s, e_type);
        
  IF print_accum THEN
      PRINT @@max_diameter as diameter;
  END;
  
  IF file_path != "" THEN
      f.println("Diameter");
      f.println(@@max_diameter);
  END;
}

CREATE QUERY tg_cycle_detection (SET<STRING> v_type, SET<STRING> e_type, INT depth, BOOL print_accum = TRUE, STRING file_path = ""){

/* RochaThatte cycle detection algorithm
This is a distributed algorithm for detecting all the cycles on large-scale directed graphs.In every iteration, the vertices send its sequences to its out-neighbors, and receive the sequences from the in-neighbors.
Stop passing the sequence (v1,v2,v3, ...) when:
1. v = v1. If v has the minimum label in the sequence, report the cycle
2. v = vi (i!=1). Do not report since this cycle is already reported in an earlier iteration
*/
  ListAccum<ListAccum<VERTEX>> @curr_list, @new_list;
  ListAccum<ListAccum<VERTEX>> @@cycles_list;
  SumAccum<INT> @sum_uid;
  FILE f (file_path);
  
  # initialization
  Active = {v_type};
  Active = SELECT s 
           FROM Active:s
           ACCUM s.@curr_list = [s];
  
  WHILE Active.size() > 0 LIMIT depth DO 
      Active = SELECT t 
               FROM Active:s -(e_type:e)-> :t
               ACCUM 
                   FOREACH sequence IN s.@curr_list DO
                       BOOL t_is_min = TRUE, 
                       IF t == sequence.get(0) THEN  # cycle detected
                           FOREACH v IN sequence DO
                               IF getvid(v) < getvid(t) THEN
                                   t_is_min = FALSE,
                                   BREAK
                               END
                           END,
                           IF t_is_min == TRUE THEN  # if it has the minimal label in the list, report 
                               IF print_accum THEN 
                                   @@cycles_list += sequence 
                               END,
                               IF file_path != "" THEN 
                                   f.println(sequence) 
                               END
                           END
                       ELSE IF sequence.contains(t) == FALSE THEN   # discard the sequences contain t
                           t.@new_list += [sequence + [t]]   # store sequences in @newList to avoid confliction with @currList
                       END
                  END
              POST-ACCUM s.@curr_list.clear(),
                         t.@curr_list = t.@new_list,
                         t.@new_list.clear()
              HAVING t.@curr_list.size() > 0;  # IF receive no sequences, deactivate it;
  END;

  IF print_accum THEN
      PRINT @@cycles_list as cycles;
  END;  
}

CREATE QUERY tg_knn_cosine_cv (SET<STRING> v_type, SET<STRING> e_type, SET<STRING> re_type, STRING weight, STRING label, INT min_k, INT max_k) RETURNS (INT){
/* Leave-one-out cross validation for selecting optimal k. 
   The input is a range of k, output is the k with highest correct prediction rate.
   Note: When one vertex has no neighbor with known label, the prediction is considered false
*/
    ListAccum<FLOAT> @@correct_rate_list; 
    ListAccum<INT> @is_correct_list; 
    ListAccum<STRING> @predicted_label_list;
    SumAccum<FLOAT> @@sum_total_score;
    INT n, k, best_k=1;
    FLOAT max_rate=0;
  
    IF max_k < min_k OR max_k < 1 THEN  // terminate if the range is invalid
        RETURN 0;
    END;
    start = {v_type};
    start = SELECT s
            FROM start:s 
            WHERE s.getAttr(label, "STRING") != ""  // get the vertices with known label
            ACCUM s.@predicted_label_list = tg_knn_cosine_cv_sub(s, e_type, re_type, label, weight, max_k)  
    // get a list of predicted label wrt different k
            POST-ACCUM 
        FOREACH pred_label IN s.@predicted_label_list DO
                    IF s.getAttr(label, "STRING") == pred_label THEN  # *vStrAttrOld*  means no neighbor with label
                        s.@is_correct_list += 1
                    ELSE
                        s.@is_correct_list += 0
                    END                   
                END;
  
     n = start.size();
     k = min_k-1;  # index starts from 0
     WHILE k < max_k DO
         @@sum_total_score = 0;
         start = SELECT s
                 FROM start:s 
                 ACCUM 
     IF s.@is_correct_list.size()==0 THEN
                         @@sum_total_score += 0  # if there is no neighbor, it is considered incorrect prediction
                     ELSE IF k >= s.@is_correct_list.size() THEN
                         @@sum_total_score += s.@is_correct_list.get(s.@is_correct_list.size()-1)   # use all neighbors it has when it is not enough  
                     ELSE 
                         @@sum_total_score += s.@is_correct_list.get(k)
                     END;
         @@correct_rate_list += @@sum_total_score / n;
         IF @@sum_total_score / n > max_rate THEN
             max_rate = @@sum_total_score / n;  # store the max correct rate in max_rate
             best_k = k+1;
         END;
         k = k+1;
     END;

     PRINT @@correct_rate_list;
     PRINT best_k;
     RETURN best_k;
}

CREATE QUERY tg_kcore(STRING v_type, STRING e_type, INT k_min = 0, INT k_max = -1, BOOL print_accum = TRUE, STRING result_attr = "", STRING file_path = "", BOOL print_all_k = FALSE, BOOL show_shells=FALSE){ 
/* An implementation of Algorithm 2 in
 * Scalable K-Core Decomposition for Static Graphs Using a Dynamic Graph Data Structure,
 * Tripathy et al., IEEE Big Data 2018.
 
 This query is only supported for a single edge type at the moment (8/13/20)
 */
  SumAccum<INT> @sum_deg;        // The number of edges v has to active vertices.
  SumAccum<INT> @sum_core;       // The core level of vertex v
  FILE f(file_path);
  INT k;                 
  k = k_min;      

  active = {v_type.*};
  active = SELECT v 
           FROM active:v // Initialize @deg 
   POST-ACCUM v.@sum_deg += v.outdegree(e_type);
  
  Q = active;
  WHILE active.size() > 0 AND (k_max == -1 OR k < k_max) DO
      deleted = SELECT v 
                FROM active:v
                WHERE v.@sum_deg <= k
                ACCUM v.@sum_core += k;
  
      active = active MINUS deleted;
      IF deleted.size() > 0 THEN                // "Remove adjacent edges"         
          U = SELECT u 
              FROM deleted:u -(e_type:e)-> :v
      ACCUM  v.@sum_deg += -1;  // Actually, reduce degree of vertices
      ELSE IF show_shells THEN 
          // Show vertices which did not satisfy kcore condition at a value of k 
          shells = Q MINUS active;
          PRINT k, shells; 
      END;
      
      IF active.size() > 0 THEN
          Q = active;
      END;
        
      //show all vertices which satisfied the condition at k.
      IF print_all_k THEN 
          PRINT k, Q as members;
      END;
      k = k + 1;

  END;
  IF file_path != "" THEN
      f.println("Vertex", "Core");
  END;
  
  IF file_path != "" OR result_attr != "" THEN
      Seed = {v_type.*};
      Seed = SELECT s 
             FROM Seed:s
             POST-ACCUM
                 IF file_path != "" THEN 
     f.println(s, s.@sum_core) 
 END,
                 IF result_attr != "" THEN 
     s.setAttr(result_attr, s.@sum_core) 
 END;
  END;
  
  IF print_accum THEN
      PRINT k, Q.size() as core_size, Q as max_core;
  END;
}

CREATE QUERY tg_article_rank (STRING v_type, STRING e_type,
 FLOAT max_change = 0.001, INT max_iter = 25, FLOAT damping = 0.85, INT top_k = 100,
 BOOL print_accum = TRUE, STRING result_attr =  "", STRING file_path = "") {
 
/*
 Compute the article rank score for each vertex in the GRAPH
 In each iteration, compute a score for each vertex:
     score = (1-damping) + damping*average outdegree*sum(received scores FROM its neighbors/average outdegree+Outdegree).
 The article Rank algorithm stops when either of the following is true:
 a) it reaches max_iter iterations;
 b) the max score change for any vertex compared to the last iteration <= max_change.
 v_type: vertex types to traverse          print_accum: print JSON output
 e_type: edge types to traverse            result_attr: INT attr to store results to
 max_iter; max #iterations                 file_path: file to write CSV output to
 top_k: #top scores to output              display_edges: output edges for visualization
 max_change: max allowed change between iterations to achieve convergence
 damping: importance of traversal vs. random teleport

 This query supports only taking in a single edge for the time being (8/13/2020).
*/

  TYPEDEF TUPLE<VERTEX Vertex_ID, FLOAT score> Vertex_Score;
  HeapAccum<Vertex_Score>(top_k, score DESC) @@top_scores_heap;
  MaxAccum<FLOAT> @@max_diff = 9999;    # max score change in an iteration
  SumAccum<FLOAT> @sum_recvd_score = 0; # sum of scores each vertex receives FROM neighbors
  SumAccum<FLOAT> @sum_score = 0.15;           # initial score for every vertex is 0.15.
  SetAccum<EDGE> @@edge_Set;             # list of all edges, if display is needed
  AvgAccum @@avg_out;
  SumAccum<INT> @sum_out_degree;
  FILE f (file_path);

# PageRank iterations	
  Start = {v_type};                     # Start with all vertices of specified type(s)
  Start = SELECT s 
          FROM Start:s 
          ACCUM 
	      s.@sum_out_degree += s.outdegree(e_type),
              @@avg_out += s.outdegree(e_type);
   
  WHILE @@max_diff > max_change 
      LIMIT max_iter DO @@max_diff = 0;
   
      V = SELECT s 
          FROM Start:s -(e_type:e)-> v_type:t
          ACCUM t.@sum_recvd_score += s.@sum_score/(@@avg_out+s.@sum_out_degree) 
	  POST-ACCUM 
	       s.@sum_score = (1.0-damping) + damping * s.@sum_recvd_score*@@avg_out,
	       s.@sum_recvd_score = 0,
               @@max_diff += abs(s.@sum_score - s.@sum_score');
   
  END; # END WHILE loop

# Output
  IF file_path != "" THEN
      f.println("Vertex_ID", "article Rank");
  END;

  V = SELECT s 
      FROM Start:s
      POST-ACCUM 
          IF result_attr != "" THEN 
              s.setAttr(result_attr, s.@sum_score) 
          END,
   
	  IF file_path != "" THEN 
              f.println(s, s.@sum_score) 
          END,
   
	  IF print_accum THEN 
              @@top_scores_heap += Vertex_Score(s, s.@sum_score) 
          END;

  IF print_accum THEN
      PRINT @@top_scores_heap;
  END;
}
CREATE QUERY tg_jaccard_nbor_ss (VERTEX source, STRING e_type, STRING rev_e_type, 
 INT top_k = 100, BOOL print_accum = TRUE, STRING similarity_edge_type = "", STRING file_path = "") {
/* 
Calculates the Jaccard Similarity between a given vertex and every other vertex.
  Jaccard similarity = intersection_size / (size_A + size_B - intersection_size)
Parameters:
 source: start vertex                           top_k: #top scores to report
 e_type: directed edge types to traverse        print_accum: print JSON output
 rev_e_type: reverse edge types to traverse     file_path: file to write CSV output to
 similarity_edge_type: edge type for storing vertex-vertex similarity scores

  This query current supports only a single edge type (not a set of types) - 8/13/20
*/

  SumAccum<INT> @sum_intersection_size, @@sum_set_size_A, @sum_set_size_B;
  SumAccum<FLOAT> @sum_similarity;
  FILE f (file_path);

  Start (ANY) = {source};
  Start = SELECT s
  FROM Start:s
  ACCUM @@sum_set_size_A += s.outdegree(e_type);

  Subjects = SELECT t
     FROM Start:s-(e_type:e)-:t;

  Others = SELECT t
   FROM Subjects:s -(rev_e_type:e)- :t
   WHERE t != source
   ACCUM 
       t.@sum_intersection_size += 1,
       t.@sum_set_size_B = t.outdegree(e_type)
   POST-ACCUM 
       t.@sum_similarity = t.@sum_intersection_size*1.0/(@@sum_set_size_A + t.@sum_set_size_B - t.@sum_intersection_size)
   ORDER BY t.@sum_similarity DESC
   LIMIT top_k;

  IF file_path != "" THEN
      f.println("Vertex1", "Vertex2", "Similarity");
  END;
  
  Others = SELECT s
   FROM Others:s
   POST-ACCUM 
       IF similarity_edge_type != "" THEN
           INSERT INTO EDGE similarity_edge_type VALUES (source, s, s.@sum_similarity)
       END,
       IF file_path != "" THEN 
           f.println(source, s, s.@sum_similarity) 
       END; 

  IF print_accum THEN
      PRINT Others[Others.@sum_similarity];
  END;
}

CREATE QUERY tg_maxflow(VERTEX source, VERTEX sink, Set<STRING> v_type, SET<STRING> e_type, 
   SET<STRING> reverse_e_type, STRING cap_attr, STRING cap_type, FLOAT min_flow_threshhold = 0.001, 
   BOOL print_accum = TRUE, BOOL display_edges = TRUE, BOOL spit_to_file = FALSE, 
   STRING file_path = "/home/tigergraph/tg_query_output.csv"){ 
  
  /*
   Maxflow algorithm. Finds the maximum amount of flow that source can pushes to sink

   source: start vertex to  every other vertex.     cap_type: capacity data type (UINT, INT,FLOAT,DOUBLE)
   sink:   end vertex                               min_flow_threshhold: smallest epsilon for flow
   v_type: vertex types to traverse                 print_accum: print JSON output
   e_type: edge types to traverse                   display_edges: visualization when print_accum && display_edges 
   reverse_e_type: reverse edge of e_type           spit_to_file: spit output to file
   cap_attr: attribute for edge capacity            file_path: file to write CSV output to
     
  */
         
  TYPEDEF TUPLE<INT prev_flow, BOOL is_forward, VERTEX prev> tb_node;
  GroupByAccum<VERTEX source, VERTEX targ, SumAccum<FLOAT> flow> @@group_by_flow_accum;
  SetAccum<VERTEX> @@curr_set;
  SetAccum<EDGE> @@edges_set;
  HeapAccum<tb_node>(1, prev_flow DESC) @trace_back_heap;
                              
  MaxAccum<FLOAT> @@max_cap_threshold;
  SumAccum<FLOAT> @@sum_max_flow = 0;
  MinAccum<FLOAT> @@min_flow;
  OrAccum @or_is_visited, @@or_is_found;
  BOOL minimum_reached = FALSE;
FILE f(file_path);
  @@max_cap_threshold = min_flow_threshhold;
         
  IF cap_type NOT IN ("UINT", "INT", "FLOAT", "DOUBLE") THEN
      PRINT "wt_type must be UINT, INT, FLOAT, or DOUBLE" AS errMsg;
      RETURN;
  END;    
  
  ##### Initialize #####
  init = {v_type};       
  init = SELECT s
         FROM init:s - (e_type:e) -> v_type:t
         ACCUM 
             FLOAT fl = 0,
             CASE cap_type 
         WHEN "UINT" THEN
             fl = e.getAttr(cap_attr, "UINT")
                 WHEN "INT" THEN
     fl = e.getAttr(cap_attr, "INT")
                 WHEN "FLOAT" THEN
                     fl = e.getAttr(cap_attr, "FLOAT")
                 WHEN "DOUBLE" THEN
                     fl = e.getAttr(cap_attr, "DOUBLE")
                 END,
             @@group_by_flow_accum += (s, t -> 0),
             IF s == source THEN 
         @@max_cap_threshold += fl 
     END;

  //used for determining minimum flow of path, s.t. minimum flow > cap_threshold
  @@max_cap_threshold = pow(3, float_to_int(log(@@max_cap_threshold)/log(3)));
  
  ##### Push one flow at a time until there is residudal graph is disconnected #####
  WHILE TRUE DO
      //initilize 
      init = {v_type};
      init = SELECT s
             FROM init:s
             POST-ACCUM s.@or_is_visited = FALSE,
                        s.@trace_back_heap = tb_node(GSQL_INT_MIN, FALSE, source);
    
      start = {source};
      start = SELECT s 
              FROM start:s 
              POST-ACCUM s.@or_is_visited = TRUE;
  
      @@or_is_found = False;
    
      //BFS to find feasible path from source -> sink
      WHILE NOT @@or_is_found AND start.size() > 0 DO
          forwd = SELECT t
                  FROM start:s - (e_type:e) -> v_type:t
                  WHERE NOT t.@or_is_visited
                  ACCUM 
                      FLOAT fl = 0,
                      CASE cap_type
                          WHEN "UINT" THEN
      fl = e.getAttr(cap_attr, "UINT")
                          WHEN "INT" THEN
      fl = e.getAttr(cap_attr, "INT")
                          WHEN "FLOAT" THEN
                              fl = e.getAttr(cap_attr, "FLOAT")
                          WHEN "DOUBLE" THEN
                              fl = e.getAttr(cap_attr, "DOUBLE")
                      END, 
                      IF fl - @@group_by_flow_accum.get(s, t).flow >= @@max_cap_threshold THEN
                          t.@trace_back_heap += tb_node(fl - @@group_by_flow_accum.get(s, t).flow, TRUE, s),
                          t.@or_is_visited += TRUE,
                          @@or_is_found += t == sink
                      END
                  HAVING t.@or_is_visited;
  
          bacwd = SELECT t
                  FROM start:s - (reverse_e_type) -> v_type:t
                  WHERE NOT t.@or_is_visited
                  ACCUM 
                      IF @@group_by_flow_accum.get(t, s).flow >= @@max_cap_threshold THEN
                          t.@trace_back_heap += tb_node(@@group_by_flow_accum.get(t, s).flow, FALSE, s),
                          t.@or_is_visited += TRUE,
                          @@or_is_found += t == sink
                      END
                  HAVING t.@or_is_visited;
  
          start = forwd UNION bacwd;
      END;
    
      //done when residual graph is disconnected                           
      IF NOT @@or_is_found AND minimum_reached THEN
          BREAK;
      END;  
    
      //reduce cap_threshold to look for more path options
      IF NOT @@or_is_found THEN
          @@max_cap_threshold = float_to_int(@@max_cap_threshold/3);
          IF @@max_cap_threshold < min_flow_threshhold THEN 
              @@max_cap_threshold = min_flow_threshhold; 
              minimum_reached = TRUE; 
          END;
                               
          CONTINUE;
      END;
    
      //find bottleneck 
      @@curr_set.clear();
      @@curr_set += sink;
      @@min_flow = GSQL_INT_MAX;
    
      WHILE NOT @@curr_set.contains(source) DO
          start = @@curr_set;
          @@curr_set.clear();
          start = SELECT s 
                  FROM start:s
                  POST-ACCUM @@min_flow += s.@trace_back_heap.top().prev_flow,
                             @@curr_set += s.@trace_back_heap.top().prev;
    
      END;
  
      @@sum_max_flow += @@min_flow;
    
      //traceback to source and update flow vertices
      @@curr_set.clear();
      @@curr_set += sink;
      WHILE NOT @@curr_set.contains(source) DO
          start = @@curr_set;
          @@curr_set.clear();
          start = SELECT s 
                  FROM start:s
                  POST-ACCUM 
      @@curr_set += s.@trace_back_heap.top().prev, 
                      CASE 
                          WHEN s.@trace_back_heap.top().is_forward THEN
                              @@group_by_flow_accum += (s.@trace_back_heap.top().prev, s -> @@min_flow)
                          ELSE 
      @@group_by_flow_accum += (s, s.@trace_back_heap.top().prev -> -@@min_flow)
                          END;
      END;
  END;

  ##### Output #####
  IF file_path != "" THEN
      f.println("Maxflow: " + to_string(@@sum_max_flow));
      f.println("From","To","Flow");
  END;                            
  start = {source};
  WHILE start.size() != 0 DO
      start = SELECT t
              FROM start:s - (e_type:e) - v_type:t
              WHERE @@group_by_flow_accum.get(s,t).flow >= min_flow_threshhold 
              ACCUM
                  IF print_accum THEN 
      @@edges_set += e 
  END,
                  IF spit_to_file THEN 
      f.println(s, t, @@group_by_flow_accum.get(s,t).flow) 
  END;
  END;
  
  IF print_accum THEN
      PRINT @@sum_max_flow;
      IF display_edges THEN
          PRINT @@edges_set; 
      END;
  END;
}

CREATE QUERY test2(/* Parameters here */) FOR GRAPH ldbc_snb { 
  /* Write query logic here */ 
  PRINT "test2 works!"; 
}
CREATE QUERY tg_shortest_ss_pos_wt (VERTEX source, SET<STRING> v_type, SET<STRING> e_type,
 STRING wt_attr, STRING wt_type, FLOAT epsilon = 0.001,BOOL print_accum = TRUE, INT output_limit = -1, 
 BOOL display_edges = FALSE, STRING result_attr = "", BOOL spit_to_file = FALSE, 
 STRING file_path = "/home/tigergraph/tg_query_output.csv") {

/*
 Single-source shortest path algorithm, with positive weight edges.
 From the source vertex, finds the weighted shortest path (FLOAT value).

 source: start vertex to  every other vertex.     print_accum: print JSON output
 v_type: vertex types to traverse                 output_limit: max #vertices to output
 e_type: edge types to traverse                   display_edges: output edges for visualization
 wt_attr: attribute for edge weights              result_attr: INT attr to store results to
 wt_type: weight data type (INT,FLOAT,DOUBLE)     spit_to_file: spit data to file 
 epsilon: min delta weight                        file_path: file to write CSV output to
*/

  MinAccum<FLOAT> @min_path = GSQL_INT_MAX;       # retain 1 shortest path
  MinAccum<FLOAT> @min_prev_path = -1;
  OrAccum @or_is_candidate = FALSE;
  SetAccum<EDGE> @@edge_set;
   
  FILE f(file_path);
   
  # Check wt_type parameter
  IF wt_type NOT IN ("UINT", "INT", "FLOAT", "DOUBLE") THEN
      PRINT "wt_type must be UINT, INT, FLOAT, or DOUBLE" AS errMsg;
      RETURN;
  END;

  ##### Initialize #####
  start = {source};
  start = SELECT s 
          FROM start:s
          POST-ACCUM s.@min_path = 0;
   
  ##### Do maximum N-1 iterations: Consider whether each edge lowers the best-known distance.
  WHILE start.size() != 0 DO 
      start = SELECT t
              FROM start:s -(e_type:e)-> v_type:t
      ACCUM 
                  t.@or_is_candidate = FALSE,
                  CASE wt_type
                      WHEN "UINT" THEN
          t.@min_path += s.@min_path + e.getAttr(wt_attr, "UINT")
                      WHEN "INT" THEN
          t.@min_path += s.@min_path + e.getAttr(wt_attr, "INT")
                      WHEN "FLOAT" THEN
                          t.@min_path += s.@min_path + e.getAttr(wt_attr, "FLOAT")
                      WHEN "DOUBLE" THEN
                          t.@min_path += s.@min_path + e.getAttr(wt_attr, "DOUBLE")
                      END
             POST-ACCUM
                 IF abs(t.@min_prev_path - t.@min_path) > epsilon THEN
                     t.@or_is_candidate = TRUE,
                     t.@min_prev_path = t.@min_path
                 END
       HAVING t.@or_is_candidate;

  END;
  ##### Output #####
  component = {v_type};
  component = SELECT s 
              FROM component:s
              WHERE s.@min_prev_path != -1
              POST-ACCUM 
          IF result_attr != "" THEN 
      s.setAttr(result_attr, s.@min_path) 
  END,  
                  IF spit_to_file THEN 
      f.println(s, s.@min_path) 
  END;
   
  IF print_accum THEN
      IF output_limit >= 0 THEN
          component = SELECT s 
              FROM component:s 
      LIMIT output_limit;
      END;
   
      PRINT component[component.@min_path as cost];
      IF display_edges THEN
          tmp = SELECT s
FROM component:s -(e_type:e)-> v_type:t
        ACCUM @@edge_set += e;
          PRINT @@edge_set;
      END;
   END;
}

CREATE QUERY tg_label_prop (SET<STRING> v_type, SET<STRING> e_type, INT max_iter, INT output_limit, BOOL print_accum = TRUE, STRING file_path = "", STRING attr = ""){
# Partition the vertices into communities, according to the Label Propagation method.
# Indicate community membership by assigning each vertex a community ID.

OrAccum @@or_changed = true;
MapAccum<INT, INT> @map;     # <communityId, numNeighbors>
MapAccum<INT, INT> @@comm_sizes_map;   # <communityId, members>
SumAccum<INT> @sum_label, @sum_num;  
FILE f (file_path);
Start = {v_type};

# Assign unique labels to each vertex
Start = SELECT s 
        FROM Start:s 
        ACCUM s.@sum_label = getvid(s);

# Propagate labels to neighbors until labels converge or the max iterations is reached
WHILE @@or_changed == true LIMIT max_iter DO
    @@or_changed = false;
    Start = SELECT s 
            FROM Start:s -(e_type:e)-> :t
            ACCUM t.@map += (s.@sum_label -> 1)  # count the occurrences of neighbor's labels
            POST-ACCUM
                INT max_v = 0,
                INT label = 0,
                # Iterate over the map to get the neighbor label that occurs most often
                FOREACH (k,v) IN t.@map DO
                    CASE WHEN v > max_v THEN
                        max_v = v,
                        label = k
                    END
                END,
                # When the neighbor search finds a label AND it is a new label
                # AND the label's count has increased, update the label.
                CASE WHEN label != 0 AND t.@sum_label != label AND max_v > t.@sum_num THEN
                    @@or_changed += true,
                    t.@sum_label = label,
                    t.@sum_num = max_v
                END,
                t.@map.clear();
END;

Start = {v_type};
Start =  SELECT s 
         FROM Start:s
         POST-ACCUM 
             IF attr != "" THEN 
                 s.setAttr(attr, s.@sum_label) 
             END,
               
             IF file_path != "" THEN 
                 f.println(s, s.@sum_label) 
             END,
               
             IF print_accum THEN 
                 @@comm_sizes_map += (s.@sum_label -> 1) 
             END
         LIMIT output_limit;

IF print_accum THEN 
    PRINT @@comm_sizes_map;
    PRINT Start[Start.@sum_label];
END;
}

CREATE QUERY tg_knn_cosine_ss (VERTEX source, SET<STRING> v_type, SET<STRING> e_type, SET<STRING> re_type, STRING weight, STRING label, INT top_k, BOOL print_accum = TRUE, STRING file_path = "", STRING attr = "") RETURNS (STRING) {

/* This query is k-nearest neighbors based on Cosine Similarity between a given vertex and every other vertex.
Cosine similarity = A \dot B / ||A|| \dot ||B||
The output is the predicted label for the source vertex, which is the majority label of its k-nearest neighbors. 
*/
  SumAccum<FLOAT> @sum_numerator, @@sum_norm1, @sum_norm2, @sum_similarity;
  MapAccum<STRING, INT> @@labels_count_map;
  FILE f(file_path);
  INT max_count = 0;
  STRING predicted_label;
  # calculate similarity and find the top k nearest neighbors
  start = {source};
  subjects = SELECT t
             FROM start:s -(e_type:e)-> :t
             ACCUM t.@sum_numerator = e.getAttr(weight, "FLOAT"),
                   @@sum_norm1 += pow(e.getAttr(weight, "FLOAT"), 2);
  neighbours = SELECT t
               FROM subjects:s -(re_type:e)-> :t
               WHERE t != source AND t.getAttr(label, "STRING") != ""    
               # only consider the neighbours with known label
               ACCUM t.@sum_numerator += s.@sum_numerator * e.getAttr(weight, "FLOAT");
  kNN = SELECT s
        FROM neighbours:s -(e_type:e)-> :t
        ACCUM s.@sum_norm2 += pow(e.getAttr(weight, "FLOAT"), 2)
        POST-ACCUM s.@sum_similarity = s.@sum_numerator/sqrt(@@sum_norm1 * s.@sum_norm2)
        ORDER BY s.@sum_similarity DESC
        LIMIT top_k; 
        
  #predict label
  kNN = SELECT s
        FROM kNN:s
        ACCUM @@labels_count_map += (s.getAttr(label, "STRING") -> 1);
        
  FOREACH (pred_label, cnt) IN @@labels_count_map DO
      IF cnt > max_count THEN
          max_count = cnt;
          predicted_label = pred_label;
      END;
  END;
  
  IF attr != "" THEN
      start = SELECT s
              FROM start:s
              POST-ACCUM s.setAttr(attr, predicted_label);
  END;
        
  IF print_accum THEN
      PRINT predicted_label;
  END;
  
  IF file_path != "" THEN
      f.println(source, predicted_label);
  END;
 
  RETURN predicted_label;
}

CREATE QUERY tg_shortest_ss_pos_wt_tb (VERTEX source, SET<STRING> v_type, SET<STRING> e_type,
 STRING wt_attr, STRING wt_type, FLOAT epsilon = 0.001,BOOL print_accum = TRUE, INT output_limit = -1, 
 BOOL display_edges = FALSE, STRING result_attr = "", BOOL spit_to_file = FALSE, 
 STRING file_path = "/home/tigergraph/tg_query_output.csv", UINT write_size = 10000){

/*
 Single-source shortest path algorithm (traceback version), with positive weight edges. 
 From the source vertex, finds the weighted shortest path (FLOAT value).

 source: start vertex to  every other vertex.     print_accum: print JSON output
 v_type: vertex types to traverse                 output_limit: max #vertices to output
 e_type: edge types to traverse                   display_edges: output edges for visualization
 wt_attr: attribute for edge weights              result_attr: INT attr to store results to
 wt_type: weight data type (INT,FLOAT,DOUBLE)     spit_to_file: spit data to file 
 epsilon: min delta weight                        file_path: file to write CSV output to
 write_size: number of paths to write concurrently
*/

  TYPEDEF TUPLE<FLOAT cost, INT length, VERTEX pred> min_tup;
  HeapAccum<min_tup>(1, cost ASC, length ASC) @min_path_heap;
  MinAccum<FLOAT> @min_prev_min_path = -1;
  ListAccum<VERTEX> @path_list;
  OrAccum @or_is_candidate = FALSE;
  SetAccum<VERTEX> @@next_tmp_set;
  ListAccum<VERTEX> @path_receiver_list, @path_sender_list;
  FILE f(file_path);
  UINT print_count = 0;
   
  # Check wt_type parameter
  IF wt_type NOT IN ("UINT", "INT", "FLOAT", "DOUBLE") THEN
      PRINT "wt_type must be UINT, INT, FLOAT, or DOUBLE" AS errMsg;
      RETURN;
  END;
  
  IF write_size == 0 THEN
      PRINT "write_size must be positive" AS errMsg;
      RETURN;
  END;
   
  ##### Initialize #####
  start = {source};
  start = SELECT s 
          FROM start:s
          POST-ACCUM s.@min_path_heap = min_tup(0, 1, s);
  

  ##### Walk one step at a time until there is no more path.
  WHILE start.size() != 0 DO 
      start = SELECT t
      FROM start:s -(e_type:e)-> v_type:t
      ACCUM 
                  t.@or_is_candidate = FALSE,
                  CASE wt_type
                      WHEN "UINT" THEN
                  t.@min_path_heap += min_tup(s.@min_path_heap.top().cost + e.getAttr(wt_attr, "UINT"), 
                          s.@min_path_heap.top().length + 1, s)
                      WHEN "INT" THEN
  t.@min_path_heap += min_tup(s.@min_path_heap.top().cost + e.getAttr(wt_attr, "INT"), 
                          s.@min_path_heap.top().length + 1, s)
                      WHEN "FLOAT" THEN
                          t.@min_path_heap += min_tup(s.@min_path_heap.top().cost + e.getAttr(wt_attr, "FLOAT"), 
                          s.@min_path_heap.top().length + 1, s)
                      WHEN "DOUBLE" THEN
                          t.@min_path_heap += min_tup(s.@min_path_heap.top().cost + e.getAttr(wt_attr, "DOUBLE"), 
                          s.@min_path_heap.top().length + 1, s)
                  END
              POST-ACCUM
              IF abs(t.@min_prev_min_path - t.@min_path_heap.top().cost) > epsilon THEN
                  t.@or_is_candidate = TRUE,
                  t.@min_prev_min_path = t.@min_path_heap.top().cost
              END
      HAVING t.@or_is_candidate;
  END;
  
  ##### Output #####
  IF spit_to_file THEN
      f.println("Vertex_ID","Distance","Shortest_Path");
  END;
  
  start_set = {source};
  component = {v_type};
  component = component MINUS start_set; 
  component = SELECT s 
              FROM component:s
              WHERE s.@min_prev_min_path != -1
              POST-ACCUM 
          IF result_attr != "" THEN 
      s.setAttr(result_attr, s.@min_path_heap.top().cost) 
  END;

  //store and write paths by batches to reduce memory consumption
  WHILE component.size() != 0 DO
      tmp = SELECT s 
            FROM component:s 
            LIMIT write_size;

      tmp1 = SELECT s
             FROM tmp:s
             ACCUM s.@path_sender_list += s,
                   s.@path_list += s;

      component = component MINUS tmp;

      //calculate path for all vertices in tmp   
      WHILE tmp1.size() != 0 DO
          tmp1 = SELECT s
                 FROM tmp1:s 
                 ACCUM
                     VERTEX pred = s.@min_path_heap.top().pred,
                     @@next_tmp_set += pred,
     
                     FOREACH vert in s.@path_sender_list DO 
                         IF pred != source THEN 
     pred.@path_receiver_list += vert 
 END,
                         vert.@path_list += pred
                     END
                 POST-ACCUM
                     s.@path_sender_list.clear();

          tmp1 = @@next_tmp_set;
          @@next_tmp_set.clear();

          tmp1 = SELECT s 
                 FROM tmp1:s
                 WHERE s != source
                 POST-ACCUM
                     s.@path_sender_list = s.@path_receiver_list,
                     s.@path_receiver_list.clear();
      END;
   
      IF spit_to_file THEN
          tmp = SELECT s
                FROM tmp:s
                POST-ACCUM
                    f.println(s, s.@min_path_heap.top().cost, s.@path_list);
      END;
      IF print_accum THEN
          IF output_limit >= 0 THEN
              IF print_count < output_limit THEN
                  tmp = SELECT s 
                FROM tmp:s 
        LIMIT output_limit - print_count; 
                  print_count = print_count + tmp.size();
                  PRINT tmp[tmp.@min_path_heap.top().cost as cost, tmp.@path_list as p];
              END;
          ELSE
              PRINT tmp[tmp.@min_path_heap.top().cost as cost, tmp.@path_list as p];
          END;
      END;

      tmp = SELECT s 
            FROM tmp:s
            POST-ACCUM
                s.@path_list.clear();
  END;  
}

CREATE QUERY tg_pagerank_pers_batch(STRING v_type, STRING e_type,
  FLOAT max_change=0.001, INT max_iter=25, FLOAT damping = 0.85, INT top_k = 100,INT batch_num,BOOL print_accum,STRING file_path) {
  /*
 Compute the pageRank score for each vertex in the GRAPH
 v_type: vertex types to traverse          print_accum: print JSON output
 e_type: edge types to traverse           
 max_iter; max #iterations                 file_path: file to write CSV output to
 top_k: #top scores to output              
 max_change: max allowed change between iterations to achieve convergence
 damping: importance of traversal vs. random teleport
*/
    
  TYPEDEF TUPLE<VERTEX ver, FLOAT score> Vertex_Score;
  HeapAccum<Vertex_Score>(top_k,score DESC) @top_scores_heap;
  MapAccum<Vertex,ListAccum<Vertex_Score>> @@res_map;
  MaxAccum<FLOAT> @@max_diff = 9999; # max score change in an iteration
  MapAccum<VERTEX,FLOAT> @received_score_map; # sum of scores each vertex receives FROM neighbors
  MapAccum<VERTEX,FLOAT> @score_map ;   # Initial score for every vertex is 0
  OrAccum @or_is_source;
  SetAccum<INT> @@end_set;
  FILE f(file_path);
  All = {v_type};#  All with a set of input vertices
        
  FOREACH batch_count in RANGE[0,batch_num-1] DO
      Start = SELECT s 
              FROM All:s 
              WHERE getvid(s)%batch_num==batch_count
              ACCUM s.@score_map +=(s->1),   # Only set score of source vertices to 1
                    s.@or_is_source = true;
                    Total = Start;
  
      WHILE @@max_diff > max_change LIMIT max_iter DO
          V_tmp = SELECT t 
                  FROM Start:s -(e_type:e)-> :t    # Only update score for activated vertices
                  ACCUM 
                      FOREACH (key,value) IN s.@score_map DO
                          IF getvid(key) in @@end_set THEN 
                              continue 
                          END,
                          FLOAT rec_score = value/s.outdegree(e_type),#/(s.outdegree(e_type);
                          t.@received_score_map += (key->rec_score) END;
          T = Start UNION V_tmp;
          Start = SELECT s 
                  FROM T:s
                  POST-ACCUM
                      IF s.@or_is_source == TRUE THEN 
                          FLOAT oldscore = s.@score_map.get(s),
                          s.@score_map+=(s->-oldscore),
                          s.@score_map+=(s->((1.0-damping) + damping *s.@received_score_map.get(s)))
                      END,
                      FOREACH (key,value) IN s.@received_score_map DO
                          IF key!=s THEN
                              FLOAT oldscore = s.@score_map.get(key),
                              s.@score_map+=(key->-oldscore),
                              s.@score_map+=(key->damping*value),
                              IF damping*value-oldscore<0.001 THEN 
                                  @@end_set+=getvid(key) 
                              END 
                          END
                      END,
                      s.@received_score_map.clear();
         Total=Total UNION T;
       
      END;
            
      Total = SELECT s 
              FROM Total:s 
              POST-ACCUM 
                  FOREACH (key,value) IN s.@score_map DO 
                      @@res_map+=(key->Vertex_Score(s,value)) 
                  END,
                  s.@score_map.clear(),s.@received_score_map.clear();
      @@end_set.clear();
  END;
  All = SELECT s 
        FROM All:s 
        POST-ACCUM 
            FOREACH i in @@res_map.get(s) DO
                s.@top_scores_heap+=i 
            END,
            IF file_path != "" THEN 
                f.println(s, s.@top_scores_heap) 
            END;
  IF print_accum THEN
      PRINT All [All.@top_scores_heap]; 
  END;
}

CREATE QUERY tg_cycle_detection_count(SET<STRING> v_type, SET<STRING> e_type, INT depth, INT batches, BOOL print_accum = TRUE)  {

/* RochaThatte cycle detection algorithm + fixes
This is a distributed algorithm for detecting all the cycles on large-scale directed graphs.In every iteration, the vertices send its sequences to its out-neighbors, and receive the sequences from the in-neighbors.
Stop passing the sequence (v1,v2,v3, ...) when:
1. v = v1. If v has the minimum label in the sequence, report the cycle
2. v = vi (i!=1). Do not report since this cycle is already reported in an earlier iteration

Added a cull so we only count a cycle once by only counting a cycle if it starts from the lowest vid in that cycle.
This allows for early exit in loop so should save memory too.
Self cycles treated completely separately to save memory and increase speed
    
  */
  
ListAccum<ListAccum<UINT>> @curr_list, @new_list; //, @@out_cycles;
SumAccum<INT> @@sum_cycles=0;
INT loop_depth ;
loop_depth = depth-1; // as we are performing the first step outside the loop

// Do first iteration, ignore self loops (can add in later)
// assuming cull where cycle not started at lowest id, so any one cycle only counted once
  
FOREACH batch_number IN RANGE[0,batches-1] DO  
    Active = {v_type};
    Active = SELECT vv 
             FROM Active:vv
             POST-ACCUM vv.@curr_list.clear()
             HAVING getvid(vv)%batches==batch_number;
    
    IF depth > 0 THEN
        Active = SELECT t 
                 FROM Active:s -(e_type)-> v_type:t
                 WHERE getvid(s) < getvid(t)   // so cannot be self-join, and asserts s must be lowest
                 ACCUM 
                     t.@curr_list += [getvid(s)] // lists of precedents
                 HAVING t.@curr_list.size() > 0;  # IF receive no sequences, deactivate it;
    END;
    WHILE Active.size() > 0 LIMIT loop_depth DO 
        Active = SELECT t 
                 FROM Active:s -(e_type:e)-> v_type:t
                 WHERE s != t
                 ACCUM 
                     INT t_id = 0,
                     t_id = getvid(t),
                     FOREACH sequence IN s.@curr_list DO
                         IF t_id < sequence.get(0) THEN // early exit, sequence started from wrong place so cull
                             CONTINUE 
                         ELSE IF t_id == sequence.get(0) THEN  # cycle detected
                             @@sum_cycles += 1
                         ELSE IF sequence.contains(t_id) == FALSE THEN   # discard the sequences contain t
                             t.@new_list += sequence + [getvid(s)]   # store sequences in @newList to avoid confliction  with @currList
                         END
                     END
                 POST-ACCUM
                     s.@curr_list.clear(),
                     t.@curr_list = t.@new_list,
                     t.@new_list.clear()
                 HAVING t.@curr_list.size() > 0;  # IF receive no sequences, deactivate it;
    END;
  
END;
  
// Self loops here
all = {v_type};
selfy = SELECT vv 
        FROM all:vv-(e_type)-v_type:tt
        WHERE vv==tt; 
@@sum_cycles+=selfy.size();

IF print_accum THEN
    PRINT @@sum_cycles AS cycles;
END;  
}

CREATE QUERY tg_influence_maximization_CELF(STRING v_type,STRING e_type,STRING weight,INT top_k,
  BOOL print_accum = True, STRING file_path = "") {  
  /* get the vertex which has maximum influence. 
  Parameters:
  v_type: vertex types to traverse                 
  e_type: edge types to traverse                   
  weight: enter weight attribute name
  top_k: report only this many top scores          
  file_path: file to write CSV output to
  print_accum: weather print the result
   */  
  TYPEDEF TUPLE<VERTEX Vertex_ID, FLOAT score> Vertex_Score;
  HeapAccum<Vertex_Score>(top_k, score DESC) @@top_scores_heap;
  OrAccum @or_in_seed;
  OrAccum @or_influenced;
  SumAccum<FLOAT> @influence_value;
  ListAccum<Vertex_Score> @@res_list;
  VERTEX k;
  FLOAT score;
  BOOL skip=FALSE;
  FILE f (file_path);
  @@top_scores_heap.resize(top_k+3);
  all = {v_type};
  start = SELECT s 
          FROM all:s-(e_type:e)-v_type:v 
          ACCUM 
              IF weight != ""  THEN 
                  s.@influence_value+=e.getAttr(weight,"FLOAT") 
              ELSE  
                  s.@influence_value+=1 
              END
          POST-ACCUM @@top_scores_heap+=Vertex_Score(s,s.@influence_value),
                     s.@influence_value=0;
  @@res_list+=@@top_scores_heap.top();
  k= @@top_scores_heap.pop().Vertex_ID;
  temp = SELECT s 
         FROM start:s-(e_type:e)-v_type:v 
         WHERE s==k
         ACCUM v.@or_influenced+=TRUE,s.@or_in_seed+=TRUE;
  k= @@top_scores_heap.pop().Vertex_ID;
  score = @@top_scores_heap.top().score;
  WHILE @@res_list.size()<top_k DO
      one = SELECT s 
            FROM all:s-(e_type:e)-v_type:v 
            WHERE s==k AND s.@or_in_seed == FALSE AND v.@or_influenced == FALSE
            ACCUM 
                IF weight != ""  THEN 
                    s.@influence_value+=e.getAttr(weight,"FLOAT") 
                ELSE  
                    s.@influence_value+=1 
                END 
            POST-ACCUM 
                IF s.@influence_value>=score THEN 
                    @@res_list+=Vertex_Score(s,s.@influence_value),skip=TRUE 
                END,
                   s.@influence_value=0;
      IF skip THEN 
          skip=FALSE;
          one = SELECT v 
                FROM all:s-(e_type:e)-v_type:v 
                WHERE s==k
                ACCUM v.@or_influenced+=TRUE,s.@or_in_seed+=TRUE;
          k= @@top_scores_heap.pop().Vertex_ID;
          score = @@top_scores_heap.top().score;
          CONTINUE;
      END;
      @@top_scores_heap.clear();
      start = SELECT s 
              FROM all:s-(e_type:e)-v_type:v 
              WHERE s.@or_in_seed == FALSE and v.@or_influenced == FALSE 
              ACCUM 
                  IF weight != ""  THEN 
                      s.@influence_value+=e.getAttr(weight,"FLOAT") 
                  ELSE  
                      s.@influence_value+=1 
                  END
              POST-ACCUM 
                  @@top_scores_heap+=Vertex_Score(s,s.@influence_value),
                  s.@influence_value=0;
      @@res_list+=@@top_scores_heap.top();
      k= @@top_scores_heap.pop().Vertex_ID;
      temp = SELECT s 
             FROM start:s-(e_type:e)-v_type:v 
             WHERE s==k
             ACCUM v.@or_influenced+=TRUE,
                   s.@or_in_seed+=TRUE;
      k= @@top_scores_heap.pop().Vertex_ID;
      score = @@top_scores_heap.top().score;
  END;
  #Output
  IF file_path != "" THEN
      f.println("Vertex_ID", "Vertex Rank");
      FOREACH i IN RANGE[0,@@res_list.size()-1] DO
          f.println(@@res_list.get(i).Vertex_ID,i+1);
      END;
  END;
  
  IF print_accum THEN
     PRINT @@res_list;
  END;
}

CREATE QUERY tg_msf (SET<STRING> v_type, SET<STRING> e_type, STRING wt_attr, STRING wt_type,
BOOL print_accum = TRUE, STRING result_attr = "", STRING file_path = ""){

  /*
  * This query identifies minimum spanning trees using the algorithm in section 6.2 of Qin et al. 2014:
  * http://www-std1.se.cuhk.edu.hk/~hcheng/paper/SIGMOD2014qin.pdf.
 
  Parameters:
  v_type: vertex types to traverse                print_accum: print JSON output
  e_type: edge types to traverse                  result_attr: INT attr to store results to
  wt_attr: attribute for edge weights             file_path: file to write CSV output to
  wt_type: weight data type (INT,FLOAT,DOUBLE)
  */
  
  TYPEDEF TUPLE <FLOAT weight, VERTEX from_v, VERTEX to_v, EDGE e, INT vid> EDGE_WEIGHT;
  MapAccum<VERTEX, VERTEX> @@parents_map;
  MapAccum<VERTEX, AndAccum<BOOL>> @@star_map;
  SumAccum<INT> @@sum_parent_changed_count;
  SetAccum<EDGE> @@result_set;
  SetAccum<EDGE_WEIGHT> @@mst_set;
  HeapAccum<EDGE_WEIGHT>(1, weight ASC, to_v ASC, vid ASC) @ew_heap;
  MinAccum<VERTEX> @min_parent; # Given a vertex v, we need to be able to send its outgoing edge info to its parent, which is only posible if we store the parent in a local accumulator.
  OrAccum @or_ignore;
  FILE f (file_path);
        
  # Check wt_type parameter
  IF wt_type NOT IN ("INT", "FLOAT", "DOUBLE") THEN
      PRINT "wt_type must be INT, FLOAT, or DOUBLE" AS errMsg;
      RETURN;
  END;
  all_v = {v_type};
  ### FOREST INITIALIZATION ###
  # For each node v, let parent p(v) = neighbor of v connected via the least-weighted edge.
  all_v = SELECT v 
          FROM all_v:v -(e_type:e)-> :u
          ACCUM
      CASE wt_type
          WHEN "INT" THEN
      v.@ew_heap += EDGE_WEIGHT(e.getAttr(wt_attr, "INT"), v,u,e, getvid(u))
  WHEN "FLOAT" THEN
      v.@ew_heap += EDGE_WEIGHT(e.getAttr(wt_attr, "FLOAT"), v,u,e, getvid(u))
  WHEN "DOUBLE" THEN
      v.@ew_heap += EDGE_WEIGHT(e.getAttr(wt_attr, "DOUBLE"), v,u,e, getvid(u))
  END
  POST-ACCUM
      @@parents_map += (v -> v.@ew_heap.top().to_v),
      @@sum_parent_changed_count += 1;

  WHILE @@sum_parent_changed_count > 0 DO
      ### BREAK CYCLES ###
      all_v = SELECT v
              FROM all_v:v
      POST-ACCUM v.@or_ignore = false;
  
      all_v = SELECT v 
              FROM all_v:v
      POST-ACCUM
          VERTEX p = @@parents_map.get(v),
          VERTEX gp = @@parents_map.get(p),
          IF v != p AND v == gp THEN
              IF (getvid(v) < getvid(p)) THEN
          @@parents_map += (v -> v),
          v.@or_ignore = TRUE
      END
          END;

      # only add edges to MST after breaking cycles to avoid double counting edges
      add_edges = SELECT v 
                  FROM all_v:v 
          WHERE v.@or_ignore == false AND v.@ew_heap.size() > 0
          POST-ACCUM
              IF file_path != "" THEN 
          @@mst_set += v.@ew_heap.top() 
      END,
              IF print_accum OR result_attr != "" THEN 
          @@result_set += v.@ew_heap.top().e 
      END;
      ### UPDATE PARENT POINTERS ###
      @@sum_parent_changed_count = 0;
      all_v = SELECT v 
              FROM all_v:v
      POST-ACCUM
          VERTEX p = @@parents_map.get(v),
          VERTEX gp = @@parents_map.get(p),
          IF (p != gp) THEN
              @@sum_parent_changed_count += 1,
      @@parents_map += (v -> gp)
          END;
      IF @@sum_parent_changed_count == 0 THEN
          BREAK;
      END;

      ### STAR DETECTION ###
      @@star_map.clear();
      # Rule 1: Let s(v) = 1 if p(v) = p(p(v))
      # Only root and depth 1 vertices will have s(v) = 1. Everything else will have s(v) = 0.
      all_v = SELECT v 
              FROM all_v:v
      POST-ACCUM
                  VERTEX parent = @@parents_map.get(v),
          IF parent == @@parents_map.get(parent) THEN
              @@star_map += (v -> true)
          ELSE
      @@star_map += (v -> false)
          END;

      # Rule 2: If s(v) = 1 but v has a grandchild u such that s(u) = 0, then s(v) = 0. This will end up updating root vertices. 
      not_star_roots = SELECT u 
                       FROM all_v:u
       WHERE @@star_map.get(u) == false
       POST-ACCUM
           @@star_map += (@@parents_map.get(@@parents_map.get(u)) -> false);

      # Rule 3: If s(p(v)) = 0, then s(v) = 0. This will end up updating vertices at depth 1 of trees.
      not_star_depth1 = SELECT u 
                        FROM all_v:u
        WHERE @@star_map.get(@@parents_map.get(u)) == false
        POST-ACCUM
            @@star_map += (u -> false);

      ### STAR HOOKING ###
      # First, we need to clear each vertex's heap and reset the local @parent.
      all_v = SELECT v 
              FROM all_v:v
      POST-ACCUM
          v.@ew_heap.clear(),
          v.@min_parent = @@parents_map.get(v);
      
      star_nodes = SELECT v 
                   FROM all_v:v -(e_type:e)-> :u
           WHERE @@star_map.get(v) == true AND v.@min_parent != u.@min_parent
           ACCUM
               VERTEX parent = v.@min_parent,
               CASE wt_type
           WHEN "INT" THEN
               parent.@ew_heap += EDGE_WEIGHT(e.getAttr(wt_attr, "INT"), v,u,e, getvid(u))
           WHEN "FLOAT" THEN
           parent.@ew_heap += EDGE_WEIGHT(e.getAttr(wt_attr, "FLOAT"), v,u,e, getvid(u))
           WHEN "DOUBLE" THEN
       parent.@ew_heap += EDGE_WEIGHT(e.getAttr(wt_attr, "DOUBLE"), v,u,e, getvid(u))
           END;
       
      updated_star_roots = SELECT v 
                           FROM all_v:v
           WHERE @@star_map.get(v) == true AND @@parents_map.get(v) == v AND v.@ew_heap.size() > 0
           POST-ACCUM
               @@parents_map += (v -> @@parents_map.get(v.@ew_heap.top().to_v));
  END;
  IF result_attr != "" THEN
      all_v = SELECT v 
              FROM all_v:v -(e_type:e)-> :u
              ACCUM
          IF e IN @@result_set THEN
              e.setAttr(result_attr, TRUE)
          ELSE
      e.setAttr(result_attr, FALSE)
          END;
  END;

  IF print_accum THEN
      PRINT @@result_set;
  END;

  IF file_path != "" THEN
      f.println("From", "To", "Weight");
      FOREACH e IN @@mst_set DO
          f.println(e.from_v, e.to_v, e.weight);
      END;
  END;
}

CREATE QUERY tg_preferential_attachment(VERTEX a, VERTEX b, SET<STRING> e_type, BOOL print_res = TRUE) { 
    /*
    This query calculates the preferential attachment value between two vertices.
    Higher the number, the closer two vertices are.

    Preferential attachment is calculated by multiplying the number of each input vertices neighbors together.

    Parameters :
        a : Input vertex one
        b : Input vertex two
        e_type: edge types to traverse. If all edge types are desired, pass in "ALL" to the set.
        print_res: Boolean of if you want to print result (True by default)
    */
    avs = {a};
    bvs = {b};

    # See if user specified edge types to traverse
    IF "ALL" NOT IN e_type THEN
        na = SELECT n 
             FROM avs -(e_type)-> :n;  # Get neighbors of vertex A
          
        nb = SELECT n 
             FROM bvs -(e_type)-> :n;  // Get neighbors of vertex B
             
    ELSE  // traverse all edge types
        na = SELECT n 
             FROM avs -()-> :n;  // Get neighbors of vertex A
             
        nb = SELECT n 
             FROM bvs -()-> :n;  // Get neighbors of vertex B
    END;

    IF print_res THEN
        PRINT na.size()*nb.size() as closeness;  // calculate and return closeness value
    END;
}

CREATE QUERY tg_knn_cosine_all(SET<STRING> v_type, SET<STRING> e_type, SET<STRING> re_type, STRING weight, STRING label, INT top_k, BOOL print_accum = TRUE, STRING file_path = "", STRING attr = ""){
/* This query is k-nearest neighbors based on Cosine Similarity on all vertices.
   The output is the predicted label for all the vertices depending on the majority label of their k-nearest neighbors.
*/
    SumAccum<STRING> @sum_predicted_label;
    FILE f (file_path);
        
    source = {v_type};        
    source = SELECT s
             FROM source:s 
             WHERE s.getAttr(label, "STRING") == ""
             POST-ACCUM s.@sum_predicted_label = tg_knn_cosine_all_sub(s, e_type,re_type, weight, label,top_k);
  
    source = SELECT s 
             FROM source:s
             POST-ACCUM
                 IF file_path != "" THEN 
                     f.println(s, s.@sum_predicted_label) 
                 END,
                 IF attr != "" THEN 
                     s.setAttr(attr, s.@sum_predicted_label) 
                 END;
      
    IF print_accum THEN
        PRINT source;
    END;  
}

set exit_on_error = "true"
